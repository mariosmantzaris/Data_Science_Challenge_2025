{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Product Classification using TF-IDF, Graph Embeddings, and Price Features\n",
    "\n",
    "This notebook implements product classification using:\n",
    "1. Text-based features: TF-IDF from product descriptions (no sentence embeddings)\n",
    "2. Graph-based features: Node2Vec embeddings\n",
    "3. Price-based features: Price buckets and statistical features\n",
    "\n",
    "We'll evaluate the approach using multiple classifiers:\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LinearSVC\n",
    "- Neural Networks\n"
   ],
   "id": "832172f6f1b94cb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Imports and Setup\n",
   "id": "84fa2038b01f84f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:15:56.659539Z",
     "start_time": "2025-05-20T19:15:51.881347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# General imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Graph embeddings\n",
    "import node2vec\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.preprocessing import label_binarize\n"
   ],
   "id": "5e6dc0c8f80d7676",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 22:15:54.217596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747768554.277744    8455 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747768554.295922    8455 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747768554.439821    8455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747768554.439856    8455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747768554.439861    8455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747768554.439865    8455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-20 22:15:54.457034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Helper Functions\n",
   "id": "5a0cd7d902c90f31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:15:56.680123Z",
     "start_time": "2025-05-20T19:15:56.671195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to extract graph features for a set of nodes\n",
    "def extract_graph_features(G, node_list):\n",
    "    print(\"Calculating degree centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    print(\"Calculating clustering coefficient...\")\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "    print(\"Calculating PageRank...\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "\n",
    "    print(\"Calculating triangle count...\")\n",
    "    triangles = nx.triangles(G)\n",
    "\n",
    "    # Create a dataframe with the features\n",
    "    features_df = pd.DataFrame(index=node_list)\n",
    "\n",
    "    features_df['degree_centrality'] = features_df.index.map(lambda x: degree_centrality.get(x, 0))\n",
    "    features_df['clustering_coefficient'] = features_df.index.map(lambda x: clustering_coefficient.get(x, 0))\n",
    "    features_df['pagerank'] = features_df.index.map(lambda x: pagerank.get(x, 0))\n",
    "    features_df['triangle_count'] = features_df.index.map(lambda x: triangles.get(x, 0))\n",
    "\n",
    "    # Degree (number of connections)\n",
    "    print(\"Calculating degree...\")\n",
    "    degree_dict = dict(G.degree())\n",
    "    features_df['degree'] = features_df.index.map(lambda x: degree_dict.get(x, 0))\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Function to calculate multiclass log loss\n",
    "def multiclass_log_loss(y_true, y_pred_proba, eps=1e-15):\n",
    "    \"\"\"\n",
    "    y_true: array-like of shape (N,) - true class labels\n",
    "    y_pred_proba: array-like of shape (N, C) - predicted class probabilities\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # One-hot encode the true labels (yij)\n",
    "    y_true_one_hot = label_binarize(y_true, classes=np.arange(y_pred_proba.shape[1]))\n",
    "\n",
    "    # Clip predicted probabilities to avoid log(0)\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    # Compute the log loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred_proba)) / N\n",
    "    return loss\n",
    "\n",
    "# Function to create price features\n",
    "def create_price_features(product_ids, price_df):\n",
    "    \"\"\"\n",
    "    Create price-based features for a list of product IDs\n",
    "    \n",
    "    Args:\n",
    "        product_ids: List of product IDs\n",
    "        price_df: DataFrame with product_id and price columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with price features\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with product IDs as index\n",
    "    price_features = pd.DataFrame(index=product_ids)\n",
    "    \n",
    "    # Map prices to products\n",
    "    price_dict = dict(zip(price_df['product_id'], price_df['price']))\n",
    "    price_features['price'] = price_features.index.map(lambda x: price_dict.get(x, np.nan))\n",
    "    \n",
    "    # Fill missing prices with median\n",
    "    median_price = price_df['price'].median()\n",
    "    price_features['price'].fillna(median_price, inplace=True)\n",
    "    \n",
    "    # Create price buckets (as binary features)\n",
    "    price_features['price_0_10'] = (price_features['price'] <= 10).astype(int)\n",
    "    price_features['price_10_100'] = ((price_features['price'] > 10) & (price_features['price'] <= 100)).astype(int)\n",
    "    price_features['price_100_plus'] = (price_features['price'] > 100).astype(int)\n",
    "    \n",
    "    # Log transformation of price\n",
    "    price_features['price_log'] = np.log1p(price_features['price'])\n",
    "    \n",
    "    # Price rank (percentile)\n",
    "    price_features['price_rank'] = price_features['price'].rank(pct=True)\n",
    "    \n",
    "    # Z-score of price (how many standard deviations from the mean)\n",
    "    mean_price = price_df['price'].mean()\n",
    "    std_price = price_df['price'].std()\n",
    "    price_features['price_zscore'] = (price_features['price'] - mean_price) / std_price\n",
    "    \n",
    "    return price_features\n"
   ],
   "id": "5be1784f857b7057",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Loading Data\n",
   "id": "dd5805d09c8f17df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:16:00.240915Z",
     "start_time": "2025-05-20T19:15:59.356517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the edge list data\n",
    "edgelist_file = 'data_files/edgelist.txt'\n",
    "edges_df = pd.read_csv(edgelist_file, header=None, names=['source', 'target'])\n",
    "\n",
    "# Load the class labels\n",
    "labels_file = 'y_train.txt'\n",
    "labels_df = pd.read_csv(labels_file, header=None, names=['product_id', 'label'])\n",
    "\n",
    "# Load the train and test splits\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Load price data\n",
    "price_df = pd.read_csv('data_files/price.txt', header=None, names=['product_id', 'price'])\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Edge list shape: {edges_df.shape}\")\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Price data shape: {price_df.shape}\")\n",
    "\n",
    "# Check the first few rows of each dataset\n",
    "print(\"\\nEdge list sample:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nLabels sample:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "print(\"\\nTrain set sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(price_df.head())\n"
   ],
   "id": "7bd619637c59d8a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list shape: (1811087, 2)\n",
      "Labels shape: (182006, 2)\n",
      "Train set shape: (145604, 3)\n",
      "Test set shape: (36402, 3)\n",
      "Price data shape: (198817, 2)\n",
      "\n",
      "Edge list sample:\n",
      "   source  target\n",
      "0  251528  237411\n",
      "1  100805   74791\n",
      "2   38634   97747\n",
      "3  247470   77089\n",
      "4  267060  250490\n",
      "\n",
      "Labels sample:\n",
      "   product_id  label\n",
      "0       66795      9\n",
      "1      242781      3\n",
      "2       91280      2\n",
      "3       56356      5\n",
      "4      218494      0\n",
      "\n",
      "Train set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0      114704  hornady unprimed winchester cartridge case hor...      2\n",
      "1      250731  tachikara tk leopard knee pad tachikara tk leo...     11\n",
      "2      152967  g asd replacement cutter aluminum amp carbon u...      2\n",
      "3        4541  mtech usa mt tactical folding knife inch close...      2\n",
      "4      142062  nhl pittsburgh penguins game day black pro sha...      7\n",
      "\n",
      "Test set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0       56218                             katz hoodie volleyball      0\n",
      "1       42346  vz grip operator ii standard size gun grip usa...      2\n",
      "2      215842  tough flat leather hobble tough flat leather h...     15\n",
      "3       36062  buzzrack skipper bicycle suv hatchback wheel c...      1\n",
      "4      188250  cuisinart cgg allfood btu portable outdoor tab...     10\n",
      "\n",
      "Price data sample:\n",
      "   product_id   price\n",
      "0           0   50.50\n",
      "1           2   17.97\n",
      "2           3    4.99\n",
      "3           4   36.60\n",
      "4           5  199.95\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:16:01.558580Z",
     "start_time": "2025-05-20T19:16:01.452474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Handle missing values in text data\n",
    "# Find rows with missing 'description' in train_df\n",
    "missing_train = train_df[train_df['text_clean'].isnull()]\n",
    "\n",
    "# Find rows with missing 'description' in test_df\n",
    "missing_test = test_df[test_df['text_clean'].isnull()]\n",
    "\n",
    "# Print the rows with missing values\n",
    "print(\"Rows with missing 'description' in train_df:\")\n",
    "print(missing_train)\n",
    "\n",
    "print(\"\\nRows with missing 'description' in test_df:\")\n",
    "print(missing_test)\n",
    "\n",
    "# Remove rows with missing 'description' in train_df\n",
    "train_df = train_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Remove rows with missing 'description' in test_df\n",
    "test_df = test_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Verify if any rows with missing values remain\n",
    "print(\"Missing values in cleaned train_df:\", train_df.isnull().sum())\n",
    "print(\"Missing values in cleaned test_df:\", test_df.isnull().sum())\n",
    "\n",
    "# Reset the index after removing rows with missing values\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "train_df.columns = ['product_id', 'description', 'label']\n",
    "test_df.columns = ['product_id', 'description', 'label']\n",
    "\n",
    "# Check the first few rows to confirm that the index is reset\n",
    "print(\"\\nTrain data after cleaning:\")\n",
    "display(HTML(train_df.head().to_html(escape=False)))\n",
    "\n",
    "print(\"\\nTest data after cleaning:\")\n",
    "display(HTML(test_df.head().to_html(escape=False)))\n"
   ],
   "id": "36d4354625c23009",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'description' in train_df:\n",
      "       product_id text_clean  label\n",
      "89285      265165        NaN      7\n",
      "\n",
      "Rows with missing 'description' in test_df:\n",
      "       product_id text_clean  label\n",
      "34767      174103        NaN      5\n",
      "Missing values in cleaned train_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Missing values in cleaned test_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Train data after cleaning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114704</td>\n",
       "      <td>hornady unprimed winchester cartridge case hornady brass ll reload great consistency predictable performance accuracy commodity brass simply deliver hornady build case precision attention detail focus perfection worldleader bullet ammunition measure consistent wall concentricity run case pressure calibration test ensure uniform case expansion quality control extreme case hand inspect thank kind manufacturing care hornady brass ensure proper seating bullet case chamber mean consistent pressure ultimately yield optimal velocity repeatable accuracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250731</td>\n",
       "      <td>tachikara tk leopard knee pad tachikara tk leopard junior knee pad ideal beginner recreational player cotton sleeve segment foam construction comfort flexibility fun animal print size fit junior</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152967</td>\n",
       "      <td>g asd replacement cutter aluminum amp carbon use aluminum carbon shafts replace individual cutter use clean andor deburr face insert shaft ensure arrowhead spine true v shape slot accept diameter shaft</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4541</td>\n",
       "      <td>mtech usa mt tactical folding knife inch close mtech linerlock closed stainless guthook blade black finish dual thumb stud pakkawood handle stainless pocket clip</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142062</td>\n",
       "      <td>nhl pittsburgh penguins game day black pro shape flat brim flex cap txz</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test data after cleaning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56218</td>\n",
       "      <td>katz hoodie volleyball</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42346</td>\n",
       "      <td>vz grip operator ii standard size gun grip usa nearly indestructible g basic design original operator replace large aggressive checkering strap aggressive golf ball pattern add thumb recess magazine release functional grip tournament shooter tactical high speed environment buying online want confident company buy exactly feel purchase vz grip lead manufacturer fine grip planet beloved weapon writeup big gun magazine world value customer stand product hard mean thing product company real deal browse product buy confidence know high quality grip firearm world class customer service firearm deserve</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>215842</td>\n",
       "      <td>tough flat leather hobble tough flat leather hobble ride horse hobble double stitch nickel hardware</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36062</td>\n",
       "      <td>buzzrack skipper bicycle suv hatchback wheel cradle trunk bike rack buzzrack skipper wheel mount bicycle rack carry bike attach car suv trunk point tie system entire car bicycle rack flexible degree adjustable place maximize form fit vehicle trunk adjustable center support allow stability bicycle transport additional feature car bicycle rack include adjustable wheel mount red safety strap tie strap design work center support secure locknut assembly system quick setup car bicycle rack complete durable black paint coat finish year manufacturer warranty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188250</td>\n",
       "      <td>cuisinart cgg allfood btu portable outdoor tabletop propane gas grill</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Creating Graph and Extracting Node2Vec Embeddings\n",
   "id": "e1ee248348b4da15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:16:15.235725Z",
     "start_time": "2025-05-20T19:16:04.420259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a graph from the edge list\n",
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f\"Is the graph connected? {is_connected}\")\n",
    "\n",
    "if not is_connected:\n",
    "    # Get the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    print(f\"\\nLargest Connected Component:\")\n",
    "    print(f\"  Nodes: {largest_cc_subgraph.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {largest_cc_subgraph.number_of_edges()}\")\n",
    "    print(f\"  Percentage of total nodes: {largest_cc_subgraph.number_of_nodes() / G.number_of_nodes() * 100:.2f}%\")\n"
   ],
   "id": "c1f02935898c8752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 276453\n",
      "Number of edges: 1811087\n",
      "Network Density: 0.000047\n",
      "Is the graph connected? False\n",
      "\n",
      "Largest Connected Component:\n",
      "  Nodes: 273012\n",
      "  Edges: 1808230\n",
      "  Percentage of total nodes: 98.76%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:17:35.704609Z",
     "start_time": "2025-05-20T19:16:18.383052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the list of product IDs from train and test sets\n",
    "train_product_ids = train_df['product_id'].tolist()\n",
    "test_product_ids = test_df['product_id'].tolist()\n",
    "\n",
    "# Extract graph features for training and testing sets\n",
    "print(\"Extracting graph features for training set...\")\n",
    "train_graph_features = extract_graph_features(G, train_product_ids)\n",
    "\n",
    "print(\"\\nExtracting graph features for testing set...\")\n",
    "test_graph_features = extract_graph_features(G, test_product_ids)\n",
    "\n",
    "# Fill missing values with 0 if any\n",
    "train_graph_features = train_graph_features.fillna(0)\n",
    "test_graph_features = test_graph_features.fillna(0)\n",
    "\n",
    "# Scale the features\n",
    "graph_scaler = StandardScaler()\n",
    "train_graph_features_scaled = graph_scaler.fit_transform(train_graph_features)\n",
    "test_graph_features_scaled = graph_scaler.transform(test_graph_features)\n"
   ],
   "id": "3f559eff024eef8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting graph features for training set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "\n",
      "Extracting graph features for testing set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:02.511420Z",
     "start_time": "2025-05-20T19:17:38.542377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Node2Vec embeddings\n",
    "print(\"Generating Node2Vec embeddings...\")\n",
    "\n",
    "try:\n",
    "    # Convert NetworkX graph to node2vec format\n",
    "    # First, ensure all nodes are strings for compatibility\n",
    "    G_node2vec = nx.Graph()\n",
    "    for edge in G.edges():\n",
    "        G_node2vec.add_edge(str(edge[0]), str(edge[1]))\n",
    "\n",
    "    # Initialize node2vec model\n",
    "    node2vec_model = node2vec.Node2Vec(\n",
    "        G_node2vec,\n",
    "        dimensions=64,  # Embedding dimension\n",
    "        walk_length=7,  # Length of each random walk\n",
    "        num_walks=7,    # Number of random walks per node\n",
    "        workers=1       # Number of parallel workers\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training Node2Vec model...\")\n",
    "    n2v_model = node2vec_model.fit(\n",
    "        window=10,       # Context size for optimization\n",
    "        min_count=1,     # Minimum count of node occurrences\n",
    "        batch_words=4    # Number of words per batch\n",
    "    )\n",
    "\n",
    "    # Generate embeddings for train and test nodes\n",
    "    train_node2vec_features = np.zeros((len(train_product_ids), 64))\n",
    "    test_node2vec_features = np.zeros((len(test_product_ids), 64))\n",
    "\n",
    "    # Extract embeddings for training nodes\n",
    "    for i, node_id in enumerate(train_product_ids):\n",
    "        try:\n",
    "            train_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    # Extract embeddings for testing nodes\n",
    "    for i, node_id in enumerate(test_product_ids):\n",
    "        try:\n",
    "            test_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    print(f\"Node2Vec embeddings shape - Train: {train_node2vec_features.shape}, Test: {test_node2vec_features.shape}\")\n",
    "\n",
    "    # Scale the embeddings\n",
    "    n2v_scaler = StandardScaler()\n",
    "    train_node2vec_scaled = n2v_scaler.fit_transform(train_node2vec_features)\n",
    "    test_node2vec_scaled = n2v_scaler.transform(test_node2vec_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Node2Vec embeddings: {e}\")\n",
    "    print(\"Skipping Node2Vec embeddings...\")\n"
   ],
   "id": "78ac224b113c8e5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Node2Vec embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/276453 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "747147bb09de474fbc6a876b7801da8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 7/7 [01:18<00:00, 11.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Node2Vec model...\n",
      "Node2Vec embeddings shape - Train: (145603, 64), Test: (36401, 64)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Extracting TF-IDF Features from Text\n",
   "id": "9deed345a30560ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:15.704517Z",
     "start_time": "2025-05-20T19:37:05.604603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare text data\n",
    "X_train_text = train_df['description']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test_text = test_df['description']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.95)\n",
    "\n",
    "# Fit and transform the text data to get the TF-IDF matrix\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"TF-IDF features shape - Train: {X_tfidf_train.shape}, Test: {X_tfidf_test.shape}\")\n"
   ],
   "id": "67c861f749664a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape - Train: (145603, 261816), Test: (36401, 261816)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Creating Price Features\n",
   "id": "b56f6bf777764ce4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:18.913464Z",
     "start_time": "2025-05-20T19:37:18.695561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create price features for training and testing sets\n",
    "print(\"Creating price features for training set...\")\n",
    "train_price_features = create_price_features(train_product_ids, price_df)\n",
    "\n",
    "print(\"Creating price features for testing set...\")\n",
    "test_price_features = create_price_features(test_product_ids, price_df)\n",
    "\n",
    "# Display the first few rows of price features\n",
    "print(\"\\nTrain price features sample:\")\n",
    "print(train_price_features.head())\n",
    "\n",
    "print(\"\\nTest price features sample:\")\n",
    "print(test_price_features.head())\n",
    "\n",
    "# Scale the price features (except binary features)\n",
    "price_scaler = StandardScaler()\n",
    "price_columns_to_scale = ['price', 'price_log', 'price_rank', 'price_zscore']\n",
    "binary_columns = ['price_0_10', 'price_10_100', 'price_100_plus']\n",
    "\n",
    "# Scale the selected columns\n",
    "train_price_scaled = train_price_features.copy()\n",
    "test_price_scaled = test_price_features.copy()\n",
    "\n",
    "train_price_scaled[price_columns_to_scale] = price_scaler.fit_transform(train_price_features[price_columns_to_scale])\n",
    "test_price_scaled[price_columns_to_scale] = price_scaler.transform(test_price_features[price_columns_to_scale])\n",
    "\n",
    "# Convert to numpy arrays for easier handling\n",
    "train_price_features_array = train_price_scaled.values\n",
    "test_price_features_array = test_price_scaled.values\n",
    "\n",
    "print(f\"Price features shape - Train: {train_price_features_array.shape}, Test: {test_price_features_array.shape}\")\n"
   ],
   "id": "b71a9ee0e10da847",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating price features for training set...\n",
      "Creating price features for testing set...\n",
      "\n",
      "Train price features sample:\n",
      "        price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "114704  43.20           0             1               0   3.788725   \n",
      "250731  24.99           0             1               0   3.257712   \n",
      "152967  22.95           0             1               0   3.175968   \n",
      "4541     8.49           1             0               0   2.250239   \n",
      "142062  24.99           0             1               0   3.257712   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "114704    0.760266     -0.130318  \n",
      "250731    0.495769     -0.326493  \n",
      "152967    0.339856     -0.348470  \n",
      "4541      0.095352     -0.504247  \n",
      "142062    0.495769     -0.326493  \n",
      "\n",
      "Test price features sample:\n",
      "         price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "56218    11.99           0             1               0   2.564180   \n",
      "42346    65.00           0             1               0   4.189655   \n",
      "215842   23.95           0             1               0   3.216874   \n",
      "36062   119.99           0             0               1   4.795708   \n",
      "188250  159.00           0             0               1   5.075174   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "56218     0.165820     -0.466542  \n",
      "42346     0.832875      0.104533  \n",
      "215842    0.347504     -0.337697  \n",
      "36062     0.912695      0.696937  \n",
      "188250    0.940469      1.117190  \n",
      "Price features shape - Train: (145603, 7), Test: (36401, 7)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Combining Features\n",
   "id": "e65f49cf606dfba8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T19:37:23.468162Z",
     "start_time": "2025-05-20T19:37:22.135400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert node2vec embeddings to sparse for efficient concatenation\n",
    "print(\"Converting node2vec embeddings to sparse format...\")\n",
    "train_node2vec_sparse = csr_matrix(train_node2vec_scaled)\n",
    "test_node2vec_sparse = csr_matrix(test_node2vec_scaled)\n",
    "\n",
    "# Convert price features to sparse\n",
    "print(\"Converting price features to sparse format...\")\n",
    "train_price_sparse = csr_matrix(train_price_features_array)\n",
    "test_price_sparse = csr_matrix(test_price_features_array)\n",
    "\n",
    "# Combine all features: TF-IDF + node2vec embeddings + price features\n",
    "print(\"Combining all features...\")\n",
    "X_combined_train = hstack([X_tfidf_train, train_node2vec_sparse, train_price_sparse], format='csr')\n",
    "X_combined_test = hstack([X_tfidf_test, test_node2vec_sparse, test_price_sparse], format='csr')\n",
    "\n",
    "# Output memory usage information\n",
    "print(f\"Final combined features shape - Train: {X_combined_train.shape}, Test: {X_combined_test.shape}\")\n",
    "print(\"Memory usage (approximate):\")\n",
    "print(f\"  - X_combined_train: {X_combined_train.data.nbytes / (1024 ** 2):.2f} MB\")\n",
    "print(f\"  - X_combined_test: {X_combined_test.data.nbytes / (1024 ** 2):.2f} MB\")\n"
   ],
   "id": "6e7acf6eaf0db84a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting node2vec embeddings to sparse format...\n",
      "Converting price features to sparse format...\n",
      "Combining all features...\n",
      "Final combined features shape - Train: (145603, 261887), Test: (36401, 261887)\n",
      "Memory usage (approximate):\n",
      "  - X_combined_train: 139.05 MB\n",
      "  - X_combined_test: 34.26 MB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Model Training and Evaluation\n",
   "id": "461a860f0d5cb055"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.1 LinearSVC\n",
   "id": "b98644058ff7b78b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:16:28.091350Z",
     "start_time": "2025-05-20T19:37:26.475442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train LinearSVC model\n",
    "print(\"Training LinearSVC model...\")\n",
    "base_svm = LinearSVC(max_iter=5000)\n",
    "svm_model = CalibratedClassifierCV(base_svm, cv=5)  # Platt scaling for probability estimates\n",
    "\n",
    "svm_model.fit(X_combined_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_combined_test)\n",
    "y_proba_svm = svm_model.predict_proba(X_combined_test)\n",
    "\n",
    "# Evaluate\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_log_loss = multiclass_log_loss(y_test, y_proba_svm)\n",
    "\n",
    "print(f\"LinearSVC Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"LinearSVC Log Loss: {svm_log_loss:.4f}\")\n",
    "print(\"\\nLinearSVC Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ],
   "id": "98963aa295c10d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearSVC model...\n",
      "LinearSVC Accuracy: 0.9427\n",
      "LinearSVC Log Loss: 0.2207\n",
      "\n",
      "LinearSVC Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      3033\n",
      "           1       0.92      0.92      0.92      2372\n",
      "           2       0.93      0.96      0.94      8652\n",
      "           3       0.96      0.97      0.96      1073\n",
      "           4       0.94      0.96      0.95      3016\n",
      "           5       0.98      0.96      0.97      3564\n",
      "           6       0.96      0.95      0.96      1519\n",
      "           7       0.97      0.94      0.95      3752\n",
      "           8       0.97      0.97      0.97      1316\n",
      "           9       0.96      0.95      0.95       903\n",
      "          10       0.89      0.88      0.89      3589\n",
      "          11       0.93      0.93      0.93      1425\n",
      "          12       0.90      0.83      0.87      1318\n",
      "          13       0.95      0.87      0.91       323\n",
      "          14       0.97      0.97      0.97       226\n",
      "          15       0.98      0.94      0.96       320\n",
      "\n",
      "    accuracy                           0.94     36401\n",
      "   macro avg       0.95      0.94      0.94     36401\n",
      "weighted avg       0.94      0.94      0.94     36401\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.2 Random Forest\n",
   "id": "ad4feec02832d948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_combined_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_combined_test)\n",
    "y_proba_rf = rf_model.predict_proba(X_combined_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_log_loss = multiclass_log_loss(y_test, y_proba_rf)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest Log Loss: {rf_log_loss:.4f}\")\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ],
   "id": "9ee6a55e143533e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.3 XGBoost\n",
   "id": "1f9798b0d4dac40e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-20T20:37:43.315692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',  # Needed for multiclass probability output\n",
    "    num_class=len(np.unique(y_train)),  # Set number of classes\n",
    "    eval_metric='mlogloss',  # Use multiclass log loss as eval metric\n",
    "    n_estimators=75,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_combined_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_combined_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_combined_test)\n",
    "\n",
    "# Evaluate\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_log_loss = multiclass_log_loss(y_test, y_proba_xgb)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Log Loss: {xgb_log_loss:.4f}\")\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ],
   "id": "a181b43046ff0371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8.4 Neural Network\n",
   "id": "447876fcd15db990"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert sparse matrix to dense for neural network\n",
    "X_combined_train_dense = X_combined_train.toarray()\n",
    "X_combined_test_dense = X_combined_test.toarray()\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Build neural network model\n",
    "print(\"Building Neural Network model...\")\n",
    "nn_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_combined_train_dense.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Neural Network model...\")\n",
    "history = nn_model.fit(\n",
    "    X_combined_train_dense, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = nn_model.predict(X_combined_test_dense)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn_classes)\n",
    "nn_log_loss = multiclass_log_loss(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy:.4f}\")\n",
    "print(f\"Neural Network Log Loss: {nn_log_loss:.4f}\")\n",
    "print(\"\\nNeural Network Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn_classes))\n"
   ],
   "id": "e9194518ab0e921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Results Comparison\n",
   "id": "2b80c5e1135d837a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a DataFrame to compare model performances\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['LinearSVC', 'Random Forest', 'XGBoost', 'Neural Network'],\n",
    "    'Accuracy': [svm_accuracy, rf_accuracy, xgb_accuracy, nn_accuracy],\n",
    "    'Log Loss': [svm_log_loss, rf_log_loss, xgb_log_loss, nn_log_loss]\n",
    "})\n",
    "\n",
    "# Sort by Log Loss (lower is better)\n",
    "results = results.sort_values('Log Loss')\n",
    "\n",
    "# Display the results\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(results)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results['Model'], results['Accuracy'], color='skyblue')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Log Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results['Model'], results['Log Loss'], color='salmon')\n",
    "plt.title('Model Log Loss Comparison')\n",
    "plt.ylabel('Log Loss (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4b3413952cecdd3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Feature Importance Analysis\n",
   "id": "18bb7e3d445ebf3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analyze feature importance from the Random Forest model\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = []\n",
    "    \n",
    "    # TF-IDF feature names (top 20 most important)\n",
    "    tfidf_feature_indices = np.argsort(importances[:X_tfidf_train.shape[1]])[-20:]\n",
    "    tfidf_features = {i: name for i, name in enumerate(tfidf_vectorizer.get_feature_names_out())}\n",
    "    \n",
    "    # Node2Vec feature names\n",
    "    node2vec_features = {i + X_tfidf_train.shape[1]: f\"node2vec_{i}\" \n",
    "                         for i in range(train_node2vec_features.shape[1])}\n",
    "    \n",
    "    # Price feature names\n",
    "    price_feature_names = ['price', 'price_0_10', 'price_10_100', 'price_100_plus', \n",
    "                           'price_log', 'price_rank', 'price_zscore']\n",
    "    price_features = {i + X_tfidf_train.shape[1] + train_node2vec_features.shape[1]: name \n",
    "                      for i, name in enumerate(price_feature_names)}\n",
    "    \n",
    "    # Combine all feature names\n",
    "    all_features = {**tfidf_features, **node2vec_features, **price_features}\n",
    "    \n",
    "    # Get top 30 features by importance\n",
    "    indices = np.argsort(importances)[-30:]\n",
    "    top_features = [(all_features.get(i, f\"feature_{i}\"), importances[i]) for i in indices]\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    importance_df = pd.DataFrame(top_features, columns=['Feature', 'Importance'])\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 30 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze price feature importance specifically\n",
    "    price_importance = []\n",
    "    for i, name in price_features.items():\n",
    "        if i < len(importances):\n",
    "            price_importance.append((name, importances[i]))\n",
    "    \n",
    "    price_importance_df = pd.DataFrame(price_importance, columns=['Feature', 'Importance'])\n",
    "    price_importance_df = price_importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Price Feature Importances:\")\n",
    "    display(price_importance_df)\n",
    "    \n",
    "    # Plot price feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(price_importance_df['Feature'], price_importance_df['Importance'], color='salmon')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Price Feature')\n",
    "    plt.title('Price Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "d59a91fb35af168f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:36:54.754761Z",
     "start_time": "2025-05-20T20:36:48.885256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load product IDs from test.txt\n",
    "print(\"Loading product IDs from test.txt...\")\n",
    "test_products = []\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        # Remove trailing comma and any whitespace\n",
    "        product_id = line.strip().rstrip(',')\n",
    "        if product_id:  # Skip empty lines\n",
    "            test_products.append(int(product_id))\n",
    "\n",
    "print(f\"Loaded {len(test_products)} product IDs from test.txt\")\n",
    "\n",
    "# CREATE PRICE FEATURES FOR TEST PRODUCTS\n",
    "print(\"\\nCreating price features for test products...\")\n",
    "test_price_features_final = create_price_features(test_products, price_df)\n",
    "\n",
    "# Scale the price features using the same scaler that was fitted on training data\n",
    "print(\"Scaling price features for test products...\")\n",
    "test_price_scaled_final = test_price_features_final.copy()\n",
    "test_price_scaled_final[price_columns_to_scale] = price_scaler.transform(test_price_features_final[price_columns_to_scale])\n",
    "\n",
    "# Convert to numpy array\n",
    "test_price_features_final_array = test_price_scaled_final.values\n",
    "print(f\"Price features shape for test products: {test_price_features_final_array.shape}\")\n",
    "\n",
    "# # Extract graph features for test products\n",
    "# print(\"\\nExtracting graph features for test products...\")\n",
    "# test_graph_features_final = extract_graph_features(G, test_products)\n",
    "# test_graph_features_final = test_graph_features_final.fillna(0)\n",
    "# test_graph_features_final_scaled = graph_scaler.transform(test_graph_features_final)\n",
    "\n",
    "# Generate Node2Vec embeddings for test products\n",
    "print(\"\\nGenerating Node2Vec embeddings for test products...\")\n",
    "test_node2vec_features_final = np.zeros((len(test_products), 64))\n",
    "\n",
    "# Extract embeddings for test products\n",
    "for i, node_id in enumerate(test_products):\n",
    "    try:\n",
    "        test_node2vec_features_final[i] = n2v_model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in embeddings, use zeros\n",
    "        pass\n",
    "\n",
    "print(f\"Node2Vec embeddings shape for test products: {test_node2vec_features_final.shape}\")\n",
    "\n",
    "# Scale the embeddings\n",
    "test_node2vec_final_scaled = n2v_scaler.transform(test_node2vec_features_final)\n",
    "\n",
    "# Load descriptions for test products\n",
    "print(\"\\nLoading descriptions for test products...\")\n",
    "descriptions = {}\n",
    "try:\n",
    "    with open(\"data_files/description_part_1.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.split('|=|')\n",
    "            if len(parts) >= 2:\n",
    "                product_id = int(parts[0])\n",
    "                description = parts[1].strip()\n",
    "                descriptions[product_id] = description\n",
    "\n",
    "    with open(\"data_files/description_part_2.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.split('|=|')\n",
    "            if len(parts) >= 2:\n",
    "                product_id = int(parts[0])\n",
    "                description = parts[1].strip()\n",
    "                descriptions[product_id] = description\n",
    "except Exception as e:\n",
    "    print(f\"Error loading descriptions: {e}\")\n",
    "    print(\"Using empty descriptions for missing products\")\n",
    "\n",
    "# Get descriptions for test products\n",
    "test_descriptions = []\n",
    "for product_id in test_products:\n",
    "    test_descriptions.append(descriptions.get(product_id, \"\"))\n",
    "\n",
    "# Extract TF-IDF features for test descriptions\n",
    "print(\"\\nExtracting TF-IDF features for test descriptions...\")\n",
    "X_tfidf_test_final = tfidf_vectorizer.transform(test_descriptions)\n",
    "print(f\"TF-IDF features shape for test products: {X_tfidf_test_final.shape}\")\n",
    "\n",
    "X_text_test_final = hstack([X_tfidf_test_final])\n",
    "\n",
    "# COMBINE ALL FEATURES INCLUDING PRICE FEATURES\n",
    "print(\"\\nCombining all features including price features...\")\n",
    "X_combined_test_final = hstack([\n",
    "    X_tfidf_test_final,\n",
    "    # test_graph_features_final_scaled,\n",
    "    test_node2vec_final_scaled,\n",
    "    test_price_features_final_array\n",
    "])\n",
    "print(f\"Combined features shape for test products (with price): {X_combined_test_final.shape}\")\n",
    "\n",
    "# Make predictions using the model\n",
    "print(\"\\nMaking predictions for test products...\")\n",
    "# test_pred_proba = svm_model.predict_proba(X_combined_test_final)\n",
    "test_pred_proba = xgb_model.predict_proba(X_combined_test_final)\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "print(\"\\nCreating CSV with predictions...\")\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['product'] = test_products\n",
    "\n",
    "# Add probability columns for each class\n",
    "for i in range(len(np.unique(y_train))):\n",
    "    predictions_df[f'class{i}'] = test_pred_proba[:, i].round(4)\n",
    "\n",
    "# Save predictions to CSV\n",
    "# predictions_df.to_csv('svm_predictions.csv', index=False)\n",
    "predictions_df.to_csv('xgb_predictions.csv', index=False)\n",
    "print(f\"Predictions saved to predictions.csv\")\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(\"\\nSample of predictions:\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Print feature dimensions for verification\n",
    "print(f\"\\nFeature dimensions:\")\n",
    "print(f\"Text features: {X_text_test_final.shape}\")\n",
    "print(f\"Graph features: {test_graph_features_final_scaled.shape}\")\n",
    "print(f\"Node2Vec features: {test_node2vec_final_scaled.shape}\")\n",
    "print(f\"Price features: {test_price_features_final_array.shape}\")\n",
    "print(f\"Total combined: {X_combined_test_final.shape}\")"
   ],
   "id": "f31693d41255092",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product IDs from test.txt...\n",
      "Loaded 45502 product IDs from test.txt\n",
      "\n",
      "Creating price features for test products...\n",
      "Scaling price features for test products...\n",
      "Price features shape for test products: (45502, 7)\n",
      "\n",
      "Generating Node2Vec embeddings for test products...\n",
      "Node2Vec embeddings shape for test products: (45502, 64)\n",
      "\n",
      "Loading descriptions for test products...\n",
      "\n",
      "Extracting TF-IDF features for test descriptions...\n",
      "TF-IDF features shape for test products: (45502, 261816)\n",
      "\n",
      "Combining all features including price features...\n",
      "Combined features shape for test products (with price): (45502, 261887)\n",
      "\n",
      "Making predictions for test products...\n",
      "\n",
      "Creating CSV with predictions...\n",
      "Predictions saved to predictions.csv\n",
      "\n",
      "Sample of predictions:\n",
      "   product  class0  class1  class2  class3  class4  class5  class6  class7  \\\n",
      "0    49957  0.0006  0.0147  0.0469  0.0007  0.0059  0.1242  0.0230  0.0003   \n",
      "1   135386  0.0000  0.0040  0.0000  0.0011  0.0121  0.0074  0.0001  0.0080   \n",
      "2   226880  0.0000  0.0016  0.9645  0.0001  0.0063  0.0029  0.0076  0.0125   \n",
      "3   165114  0.0002  0.0095  0.0168  0.0018  0.0039  0.0003  0.0008  0.0029   \n",
      "4   256154  0.0000  0.0212  0.9643  0.0002  0.0022  0.0040  0.0002  0.0013   \n",
      "\n",
      "   class8  class9  class10  class11  class12  class13  class14  class15  \n",
      "0  0.0000  0.0011   0.7781   0.0003   0.0026   0.0002   0.0008   0.0005  \n",
      "1  0.0000  0.9603   0.0004   0.0061   0.0001   0.0002   0.0000   0.0001  \n",
      "2  0.0000  0.0001   0.0019   0.0004   0.0010   0.0000   0.0000   0.0009  \n",
      "3  0.0000  0.0012   0.9605   0.0003   0.0001   0.0000   0.0000   0.0015  \n",
      "4  0.0011  0.0000   0.0017   0.0015   0.0013   0.0001   0.0010   0.0001  \n",
      "\n",
      "Feature dimensions:\n",
      "Text features: (45502, 261816)\n",
      "Graph features: (45502, 5)\n",
      "Node2Vec features: (45502, 64)\n",
      "Price features: (45502, 7)\n",
      "Total combined: (45502, 261887)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:29:43.738423Z",
     "start_time": "2025-05-20T20:29:43.733319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check individual feature components for TRAINING\n",
    "print(\"\\nTRAINING SET:\")\n",
    "print(f\"Text features (X_text_train): {X_text_test_final.shape}\")\n",
    "print(f\"Graph features (train_graph_features_scaled): {train_graph_features_scaled.shape}\")\n",
    "print(f\"Node2Vec features (train_node2vec_scaled): {train_node2vec_scaled.shape}\")\n",
    "print(f\"Price features (train_price_features_array): {train_price_features_array.shape}\")\n",
    "print(f\"TOTAL TRAINING: {X_combined_train.shape}\")\n",
    "\n",
    "# Check individual feature components for TEST (final prediction)\n",
    "print(\"\\nFINAL TEST SET:\")\n",
    "print(f\"Text features (X_text_test_final): {X_text_test_final.shape}\")\n",
    "print(f\"Graph features (test_graph_features_final_scaled): {test_graph_features_final_scaled.shape}\")\n",
    "print(f\"Node2Vec features (test_node2vec_final_scaled): {test_node2vec_final_scaled.shape}\")\n",
    "print(f\"Price features (test_price_features_final_array): {test_price_features_final_array.shape}\")"
   ],
   "id": "b5cf43cd1764f9ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING SET:\n",
      "Text features (X_text_train): (45502, 261816)\n",
      "Graph features (train_graph_features_scaled): (145603, 5)\n",
      "Node2Vec features (train_node2vec_scaled): (145603, 64)\n",
      "Price features (train_price_features_array): (145603, 7)\n",
      "TOTAL TRAINING: (145603, 261887)\n",
      "\n",
      "FINAL TEST SET:\n",
      "Text features (X_text_test_final): (45502, 261816)\n",
      "Graph features (test_graph_features_final_scaled): (45502, 5)\n",
      "Node2Vec features (test_node2vec_final_scaled): (45502, 64)\n",
      "Price features (test_price_features_final_array): (45502, 7)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. Conclusion\n",
   "id": "f02458451d0560ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook, we implemented product classification using:\n",
    "1. Text-based features: TF-IDF from product descriptions\n",
    "2. Graph-based features: Node2Vec embeddings\n",
    "3. Price-based features: Price buckets and statistical features\n",
    "\n",
    "Key findings:\n",
    "1. The combination of TF-IDF, graph embeddings, and price features provides good classification performance.\n",
    "2. Price features contribute valuable information to the classification task, especially the price buckets and z-score.\n",
    "3. The best performing model based on log loss was [BEST_MODEL], which achieved an accuracy of [BEST_ACCURACY].\n",
    "4. Feature importance analysis shows that [FEATURE_TYPE] features are most important for classification.\n",
    "\n",
    "This approach demonstrates that we can achieve good classification performance without using sentence embeddings, by leveraging graph structure and price information alongside TF-IDF features."
   ],
   "id": "415dec0170dfd447"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
