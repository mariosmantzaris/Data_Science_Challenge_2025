{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ce6e655957c097",
   "metadata": {},
   "source": [
    "# Product Classification using TF-IDF, Graph Features, and Ensemble Models with Stacking\n",
    "\n",
    "This notebook implements product classification by combining approaches from two different notebooks:\n",
    "1. Graph features from `tfidf_node2vec_classification.ipynb`\n",
    "2. TF-IDF implementation from `Data_Challenge_TFIDF.ipynb`\n",
    "3. Models: CalibratedCSVC and XGBoost from `Data_Challenge_TFIDF.ipynb`, Neural Network and Ensemble from `tfidf_node2vec_classification.ipynb`\n",
    "4. Focus on optimizing log loss like in `Data_Challenge_TFIDF.ipynb`\n",
    "5. Test predictions implementation from `Data_Challenge_TFIDF.ipynb`\n",
    "6. Uses stacking instead of weighted averaging for ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fdd3cf718c621",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7803aed95ec0c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 17:50:09.556000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748703009.614841   27540 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748703009.632049   27540 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748703009.765498   27540 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748703009.765518   27540 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748703009.765522   27540 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748703009.765524   27540 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 17:50:09.780645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Graph embeddings\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# For stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8f2873dd83018",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7e6c2e4f3c4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create price features (from tfidf_node2vec_classification.ipynb)\n",
    "def create_price_features(product_ids, price_df):\n",
    "    \"\"\"\n",
    "    Create price-based features for a list of product IDs\n",
    "\n",
    "    Args:\n",
    "        product_ids: List of product IDs\n",
    "        price_df: DataFrame with product_id and price columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with price features\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with product IDs as index\n",
    "    price_features = pd.DataFrame(index=product_ids)\n",
    "\n",
    "    # Map prices to products\n",
    "    price_dict = dict(zip(price_df['product_id'], price_df['price']))\n",
    "    price_features['price'] = price_features.index.map(lambda x: price_dict.get(x, np.nan))\n",
    "\n",
    "    # Fill missing prices with median\n",
    "    median_price = price_df['price'].median()\n",
    "    price_features['price'].fillna(median_price, inplace=True)\n",
    "\n",
    "    # Create price buckets (as binary features)\n",
    "    price_features['price_0_10'] = (price_features['price'] <= 10).astype(int)\n",
    "    price_features['price_10_100'] = ((price_features['price'] > 10) & (price_features['price'] <= 100)).astype(int)\n",
    "    price_features['price_100_plus'] = (price_features['price'] > 100).astype(int)\n",
    "\n",
    "    # Log transformation of price\n",
    "    price_features['price_log'] = np.log1p(price_features['price'])\n",
    "\n",
    "    # Price rank (percentile)\n",
    "    price_features['price_rank'] = price_features['price'].rank(pct=True)\n",
    "\n",
    "    # Z-score of price (how many standard deviations from the mean)\n",
    "    mean_price = price_df['price'].mean()\n",
    "    std_price = price_df['price'].std()\n",
    "    price_features['price_zscore'] = (price_features['price'] - mean_price) / std_price\n",
    "\n",
    "    return price_features\n",
    "\n",
    "# Function to extract graph features for a set of nodes (from tfidf_node2vec_classification.ipynb)\n",
    "def extract_graph_features(G, node_list):\n",
    "    print(\"Calculating degree centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    print(\"Calculating clustering coefficient...\")\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "    print(\"Calculating PageRank...\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "\n",
    "    print(\"Calculating triangle count...\")\n",
    "    triangles = nx.triangles(G)\n",
    "\n",
    "    # Create a dataframe with the features\n",
    "    features_df = pd.DataFrame(index=node_list)\n",
    "\n",
    "    features_df['degree_centrality'] = features_df.index.map(lambda x: degree_centrality.get(str(x), 0))\n",
    "    features_df['clustering_coefficient'] = features_df.index.map(lambda x: clustering_coefficient.get(str(x), 0))\n",
    "    features_df['pagerank'] = features_df.index.map(lambda x: pagerank.get(str(x), 0))\n",
    "    features_df['triangle_count'] = features_df.index.map(lambda x: triangles.get(str(x), 0))\n",
    "\n",
    "    # Degree (number of connections)\n",
    "    print(\"Calculating degree...\")\n",
    "    degree_dict = dict(G.degree())\n",
    "    features_df['degree'] = features_df.index.map(lambda x: degree_dict.get(str(x), 0))\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Text preprocessing function with lemmatization (from Data_Challenge_TFIDF.ipynb)\n",
    "def clean_text_with_lemma(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Use spaCy to tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if token.lemma_.lower() not in STOP_WORDS\n",
    "        and not token.is_punct\n",
    "        and not token.is_space\n",
    "        and not token.like_num\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to calculate multiclass log loss (from both notebooks)\n",
    "def multiclass_log_loss(y_true, y_pred_proba, eps=1e-15):\n",
    "    \"\"\"\n",
    "    y_true: array-like of shape (N,) - true class labels\n",
    "    y_pred_proba: array-like of shape (N, C) - predicted class probabilities\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # One-hot encode the true labels (yij)\n",
    "    y_true_one_hot = label_binarize(y_true, classes=np.arange(y_pred_proba.shape[1]))\n",
    "\n",
    "    # Clip predicted probabilities to avoid log(0)\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    # Compute the log loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred_proba)) / N\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894fc4db508fc43",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec2a6135cc76fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list shape: (1811087, 2)\n",
      "Labels shape: (182006, 2)\n",
      "Train set shape: (145604, 3)\n",
      "Test set shape: (36402, 3)\n",
      "Price data shape: (198817, 2)\n",
      "\n",
      "Edge list sample:\n",
      "   source  target\n",
      "0  251528  237411\n",
      "1  100805   74791\n",
      "2   38634   97747\n",
      "3  247470   77089\n",
      "4  267060  250490\n",
      "\n",
      "Labels sample:\n",
      "   product_id  label\n",
      "0       66795      9\n",
      "1      242781      3\n",
      "2       91280      2\n",
      "3       56356      5\n",
      "4      218494      0\n",
      "\n",
      "Train set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0      114704  hornady unprimed winchester cartridge case hor...      2\n",
      "1      250731  tachikara tk leopard knee pad tachikara tk leo...     11\n",
      "2      152967  g asd replacement cutter aluminum amp carbon u...      2\n",
      "3        4541  mtech usa mt tactical folding knife inch close...      2\n",
      "4      142062  nhl pittsburgh penguins game day black pro sha...      7\n",
      "\n",
      "Test set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0       56218                             katz hoodie volleyball      0\n",
      "1       42346  vz grip operator ii standard size gun grip usa...      2\n",
      "2      215842  tough flat leather hobble tough flat leather h...     15\n",
      "3       36062  buzzrack skipper bicycle suv hatchback wheel c...      1\n",
      "4      188250  cuisinart cgg allfood btu portable outdoor tab...     10\n",
      "\n",
      "Price data sample:\n",
      "   product_id   price\n",
      "0           0   50.50\n",
      "1           2   17.97\n",
      "2           3    4.99\n",
      "3           4   36.60\n",
      "4           5  199.95\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model for text preprocessing\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the edge list data\n",
    "edgelist_file = 'data_files/edgelist.txt'\n",
    "edges_df = pd.read_csv(edgelist_file, header=None, names=['source', 'target'])\n",
    "\n",
    "# Load the class labels\n",
    "labels_file = 'y_train.txt'\n",
    "labels_df = pd.read_csv(labels_file, header=None, names=['product_id', 'label'])\n",
    "\n",
    "# Load the train and test splits\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Load price data\n",
    "price_df = pd.read_csv('data_files/price.txt', header=None, names=['product_id', 'price'])\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Edge list shape: {edges_df.shape}\")\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Price data shape: {price_df.shape}\")\n",
    "\n",
    "# Check the first few rows of each dataset\n",
    "print(\"\\nEdge list sample:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nLabels sample:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "print(\"\\nTrain set sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(price_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173ad808332bbe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'description' in train_df:\n",
      "       product_id text_clean  label\n",
      "89285      265165        NaN      7\n",
      "\n",
      "Rows with missing 'description' in test_df:\n",
      "       product_id text_clean  label\n",
      "34767      174103        NaN      5\n",
      "Missing values in cleaned train_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Missing values in cleaned test_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Loading product IDs from test.txt...\n",
      "Loaded 45502 product IDs from test.txt\n",
      "First 10 product IDs: ['49957', '135386', '226880', '165114', '256154', '254193', '20830', '46170', '19248', '158023']\n",
      "Getting descriptions\n",
      "Apply cleaning\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in text data\n",
    "# Find rows with missing 'description' in train_df\n",
    "missing_train = train_df[train_df['text_clean'].isnull()]\n",
    "\n",
    "# Find rows with missing 'description' in test_df\n",
    "missing_test = test_df[test_df['text_clean'].isnull()]\n",
    "\n",
    "# Print the rows with missing values\n",
    "print(\"Rows with missing 'description' in train_df:\")\n",
    "print(missing_train)\n",
    "\n",
    "print(\"\\nRows with missing 'description' in test_df:\")\n",
    "print(missing_test)\n",
    "\n",
    "# Remove rows with missing 'description' in train_df\n",
    "train_df = train_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Remove rows with missing 'description' in test_df\n",
    "test_df = test_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Verify if any rows with missing values remain\n",
    "print(\"Missing values in cleaned train_df:\", train_df.isnull().sum())\n",
    "print(\"Missing values in cleaned test_df:\", test_df.isnull().sum())\n",
    "\n",
    "# Reset the index after removing rows with missing values\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Load product IDs from test.txt for final predictions\n",
    "print(\"Loading product IDs from test.txt...\")\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_products = [line.strip().rstrip(',') for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(test_products)} product IDs from test.txt\")\n",
    "print(\"First 10 product IDs:\", test_products[:10])\n",
    "\n",
    "# Load descriptions for all products\n",
    "print(\"Getting descriptions\")\n",
    "descriptions = dict()\n",
    "with open(\"data_files/description_part_1.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "with open(\"data_files/description_part_2.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "# Get descriptions for test products\n",
    "test_text = []\n",
    "for i in test_products:\n",
    "    try:\n",
    "        test_text.append(descriptions[int(i)])\n",
    "    except (KeyError, ValueError):\n",
    "        test_text.append(\"\")  # Empty string for missing descriptions\n",
    "\n",
    "# Apply the cleaning function to test data\n",
    "print(\"Apply cleaning\")\n",
    "test_text_cleaned = [clean_text_with_lemma(text) for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504454ec7b57f18a",
   "metadata": {},
   "source": [
    "## 4. Creating Graph and Extracting Graph Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5560393c8df934e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Number of nodes: 276453\n",
      "Number of edges: 1811087\n",
      "Network Density: 0.000047\n",
      "Is the graph connected? False\n",
      "\n",
      "Largest Connected Component:\n",
      "  Nodes: 273012\n",
      "  Edges: 1808230\n",
      "  Percentage of total nodes: 98.76%\n",
      "\n",
      "Extracting graph features for training set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "\n",
      "Extracting graph features for testing set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "Loading Node2Vec embeddings...\n",
      "Loading existing Node2Vec model from disk...\n",
      "Node2Vec embeddings shape - Train: (145603, 128), Test: (36401, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create a graph from the edge list\n",
    "print(\"Creating graph\")\n",
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f\"Is the graph connected? {is_connected}\")\n",
    "\n",
    "if not is_connected:\n",
    "    # Get the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    print(f\"\\nLargest Connected Component:\")\n",
    "    print(f\"  Nodes: {largest_cc_subgraph.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {largest_cc_subgraph.number_of_edges()}\")\n",
    "    print(f\"  Percentage of total nodes: {largest_cc_subgraph.number_of_nodes() / G.number_of_nodes() * 100:.2f}%\")\n",
    "\n",
    "# Get the list of product IDs from train and test sets\n",
    "train_product_ids = train_df['product_id'].tolist()\n",
    "test_product_ids = test_df['product_id'].tolist()\n",
    "\n",
    "# Extract graph features for training and testing sets\n",
    "print(\"\\nExtracting graph features for training set...\")\n",
    "train_graph_features = extract_graph_features(G, train_product_ids)\n",
    "\n",
    "print(\"\\nExtracting graph features for testing set...\")\n",
    "test_graph_features = extract_graph_features(G, test_product_ids)\n",
    "\n",
    "# Fill missing values with 0 if any\n",
    "train_graph_features = train_graph_features.fillna(0)\n",
    "test_graph_features = test_graph_features.fillna(0)\n",
    "\n",
    "# Scale the features\n",
    "graph_scaler = StandardScaler()\n",
    "train_graph_features_scaled = graph_scaler.fit_transform(train_graph_features)\n",
    "test_graph_features_scaled = graph_scaler.transform(test_graph_features)\n",
    "\n",
    "# Generate Node2Vec embeddings\n",
    "model_path = \"node2vec.model\"\n",
    "print(\"Loading Node2Vec embeddings...\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing Node2Vec model from disk...\")\n",
    "        n2v_model = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        # Convert NetworkX graph to node2vec format\n",
    "        # First, ensure all nodes are strings for compatibility\n",
    "        G_node2vec = nx.Graph()\n",
    "        for edge in G.edges():\n",
    "            G_node2vec.add_edge(str(edge[0]), str(edge[1]))\n",
    "\n",
    "        # Initialize node2vec model\n",
    "        node2vec_model = node2vec.Node2Vec(\n",
    "            G_node2vec,\n",
    "            dimensions=128,  # Embedding dimension\n",
    "            walk_length=10,  # Length of each random walk\n",
    "            num_walks=10,    # Number of random walks per node\n",
    "            workers=1       # Number of parallel workers\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training Node2Vec model...\")\n",
    "        n2v_model = node2vec_model.fit(\n",
    "            window=10,       # Context size for optimization\n",
    "            min_count=1,     # Minimum count of node occurrences\n",
    "            batch_words=4    # Number of words per batch\n",
    "        )\n",
    "\n",
    "        n2v_model.save(model_path)\n",
    "\n",
    "    # Generate embeddings for train and test nodes\n",
    "    train_node2vec_features = np.zeros((len(train_product_ids), 128))\n",
    "    test_node2vec_features = np.zeros((len(test_product_ids), 128))\n",
    "\n",
    "    # Extract embeddings for training nodes\n",
    "    for i, node_id in enumerate(train_product_ids):\n",
    "        try:\n",
    "            train_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    # Extract embeddings for testing nodes\n",
    "    for i, node_id in enumerate(test_product_ids):\n",
    "        try:\n",
    "            test_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    print(f\"Node2Vec embeddings shape - Train: {train_node2vec_features.shape}, Test: {test_node2vec_features.shape}\")\n",
    "\n",
    "    # Scale the embeddings\n",
    "    n2v_scaler = StandardScaler()\n",
    "    train_node2vec_scaled = n2v_scaler.fit_transform(train_node2vec_features)\n",
    "    test_node2vec_scaled = n2v_scaler.transform(test_node2vec_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Node2Vec embeddings: {e}\")\n",
    "    print(\"Skipping Node2Vec embeddings...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d07ee3ce5a111",
   "metadata": {},
   "source": [
    "## 5. Extracting TF-IDF Features from Text (using approach from Data_Challenge_TFIDF.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b027e074a34dd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TF-IDF\n",
      "TF-IDF features shape - Train: (145603, 2060013), Test: (36401, 2060013)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing TF-IDF\")\n",
    "# Initialize the TfidfVectorizer with parameters from Data_Challenge_TFIDF.ipynb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95, sublinear_tf=True, norm='l2')\n",
    "\n",
    "# Fit and transform the text data to get the TF-IDF matrix\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "X_tfidf_test_comp = tfidf_vectorizer.transform(test_text_cleaned)  # For final predictions\n",
    "\n",
    "print(f\"TF-IDF features shape - Train: {X_tfidf_train.shape}, Test: {X_tfidf_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f9847ce7cd798",
   "metadata": {},
   "source": [
    "## 6. Creating Price Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6451ce10ec10535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating price features for training set...\n",
      "Creating price features for testing set...\n",
      "\n",
      "Train price features sample:\n",
      "        price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "114704  43.20           0             1               0   3.788725   \n",
      "250731  24.99           0             1               0   3.257712   \n",
      "152967  22.95           0             1               0   3.175968   \n",
      "4541     8.49           1             0               0   2.250239   \n",
      "142062  24.99           0             1               0   3.257712   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "114704    0.760266     -0.130318  \n",
      "250731    0.495769     -0.326493  \n",
      "152967    0.339856     -0.348470  \n",
      "4541      0.095352     -0.504247  \n",
      "142062    0.495769     -0.326493  \n",
      "\n",
      "Test price features sample:\n",
      "         price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "56218    11.99           0             1               0   2.564180   \n",
      "42346    65.00           0             1               0   4.189655   \n",
      "215842   23.95           0             1               0   3.216874   \n",
      "36062   119.99           0             0               1   4.795708   \n",
      "188250  159.00           0             0               1   5.075174   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "56218     0.165820     -0.466542  \n",
      "42346     0.832875      0.104533  \n",
      "215842    0.347504     -0.337697  \n",
      "36062     0.912695      0.696937  \n",
      "188250    0.940469      1.117190  \n",
      "Price features shape - Train: (145603, 7), Test: (36401, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create price features for training and testing sets\n",
    "print(\"Creating price features for training set...\")\n",
    "train_price_features = create_price_features(train_product_ids, price_df)\n",
    "\n",
    "print(\"Creating price features for testing set...\")\n",
    "test_price_features = create_price_features(test_product_ids, price_df)\n",
    "\n",
    "# Display the first few rows of price features\n",
    "print(\"\\nTrain price features sample:\")\n",
    "print(train_price_features.head())\n",
    "\n",
    "print(\"\\nTest price features sample:\")\n",
    "print(test_price_features.head())\n",
    "\n",
    "# Scale the price features (except binary features)\n",
    "price_scaler = StandardScaler()\n",
    "price_columns_to_scale = ['price', 'price_log', 'price_rank', 'price_zscore']\n",
    "binary_columns = ['price_0_10', 'price_10_100', 'price_100_plus']\n",
    "\n",
    "# Scale the selected columns\n",
    "train_price_scaled = train_price_features.copy()\n",
    "test_price_scaled = test_price_features.copy()\n",
    "\n",
    "train_price_scaled[price_columns_to_scale] = price_scaler.fit_transform(train_price_features[price_columns_to_scale])\n",
    "test_price_scaled[price_columns_to_scale] = price_scaler.transform(test_price_features[price_columns_to_scale])\n",
    "\n",
    "# Convert to numpy arrays for easier handling\n",
    "train_price_features_array = train_price_scaled.values\n",
    "test_price_features_array = test_price_scaled.values\n",
    "\n",
    "print(f\"Price features shape - Train: {train_price_features_array.shape}, Test: {test_price_features_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26de5efd2c66ed",
   "metadata": {},
   "source": [
    "## 7. Combining Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943888304cdb9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting node2vec embeddings to sparse format...\n",
      "Converting graph features to sparse format...\n",
      "Converting price features to sparse format...\n",
      "Combining all features...\n",
      "Final combined features shape - Train: (145603, 2060153), Test: (36401, 2060153)\n",
      "Memory usage (approximate):\n",
      "  - X_combined_train: 230.32 MB\n",
      "  - X_combined_test: 55.17 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert node2vec embeddings to sparse for efficient concatenation\n",
    "print(\"Converting node2vec embeddings to sparse format...\")\n",
    "train_node2vec_sparse = csr_matrix(train_node2vec_scaled)\n",
    "test_node2vec_sparse = csr_matrix(test_node2vec_scaled)\n",
    "\n",
    "# Convert graph features to sparse\n",
    "print(\"Converting graph features to sparse format...\")\n",
    "train_graph_sparse = csr_matrix(train_graph_features_scaled)\n",
    "test_graph_sparse = csr_matrix(test_graph_features_scaled)\n",
    "\n",
    "# Convert price features to sparse\n",
    "print(\"Converting price features to sparse format...\")\n",
    "train_price_sparse = csr_matrix(train_price_features_array)\n",
    "test_price_sparse = csr_matrix(test_price_features_array)\n",
    "\n",
    "# Combine all features: TF-IDF + graph features + node2vec embeddings + price features\n",
    "print(\"Combining all features...\")\n",
    "X_combined_train = hstack([X_tfidf_train, train_graph_sparse, train_node2vec_sparse, train_price_sparse], format='csr')\n",
    "X_combined_test = hstack([X_tfidf_test, test_graph_sparse, test_node2vec_sparse, test_price_sparse], format='csr')\n",
    "\n",
    "# Output memory usage information\n",
    "print(f\"Final combined features shape - Train: {X_combined_train.shape}, Test: {X_combined_test.shape}\")\n",
    "print(\"Memory usage (approximate):\")\n",
    "print(f\"  - X_combined_train: {X_combined_train.data.nbytes / (1024 ** 2):.2f} MB\")\n",
    "print(f\"  - X_combined_test: {X_combined_test.data.nbytes / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda7535d0bbbaf3",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f89c3de50ff20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1000000 features\n",
      "(145603, 1000000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "k_feat = 1000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected = feature_selector.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected = feature_selector.transform(X_combined_test)\n",
    "print(X_combined_train_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244ba24084d35c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 2000000 features\n",
      "(145603, 2000000)\n"
     ]
    }
   ],
   "source": [
    "# We trained the LinearSVC on 2m features so we use another feature selector\n",
    "k_feat = 2000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector_svm = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected_svm = feature_selector_svm.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected_svm = feature_selector_svm.transform(X_combined_test)\n",
    "print(X_combined_train_selected_svm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8feb387e8a36112",
   "metadata": {},
   "source": [
    "### 8.1 Loading Pre-trained Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfcdb1ba8a07ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LinearSVC model...\n",
      "LinearSVC Accuracy: 0.9439, Log Loss: 0.2128\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained models\n",
    "print(\"Loading LinearSVC model...\")\n",
    "svm_model = joblib.load('linear_svc_model.pkl')\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_combined_test_selected_svm)\n",
    "y_proba_svm = svm_model.predict_proba(X_combined_test_selected_svm)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_log_loss = multiclass_log_loss(y_test, y_proba_svm)\n",
    "\n",
    "print(f\"LinearSVC Accuracy: {svm_accuracy:.4f}, Log Loss: {svm_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e15f78-b449-472b-9650-226fc547569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XGBoost model...\n",
      "XGBoost Accuracy: 0.9283, Log Loss: 0.2413\n"
     ]
    }
   ],
   "source": [
    "# Load XGBoost model\n",
    "print(\"Loading XGBoost model...\")\n",
    "xgb_model = joblib.load('xgb_model.pkl')\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_combined_test_selected)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_combined_test_selected)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_log_loss = multiclass_log_loss(y_test, y_proba_xgb)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}, Log Loss: {xgb_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edac4eec-67fd-475b-86e0-e2f14bab772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Neural Network model...\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 139ms/step\n",
      "Neural Network Accuracy: 0.9323, Log Loss: 0.2617\n"
     ]
    }
   ],
   "source": [
    "# Load Neural Network model\n",
    "print(\"Loading Neural Network model...\")\n",
    "nn_model = load_model('neural_network_model.keras')\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_combined_test_selected)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn_classes)\n",
    "nn_log_loss = multiclass_log_loss(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy:.4f}, Log Loss: {nn_log_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b8fedada60688",
   "metadata": {},
   "source": [
    "### 8.2 Implementing Stacking Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db45a8ea5392ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training data for stacking...\n",
      "Generating base model predictions for stacking...\n",
      "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 137ms/step\n",
      "Preparing meta-features for stacking...\n",
      "Generating base model predictions on test set...\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 137ms/step\n",
      "Preparing meta-features for test set...\n",
      "Training meta-model for stacking...\n",
      "Making predictions with stacking ensemble...\n",
      "Stacking Ensemble Accuracy: 0.9431\n",
      "Stacking Ensemble Log Loss: 0.2821\n",
      "\n",
      "Stacking Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      3033\n",
      "           1       0.91      0.92      0.91      2372\n",
      "           2       0.93      0.95      0.94      8652\n",
      "           3       0.96      0.97      0.97      1073\n",
      "           4       0.96      0.96      0.96      3016\n",
      "           5       0.98      0.96      0.97      3564\n",
      "           6       0.96      0.96      0.96      1519\n",
      "           7       0.96      0.95      0.95      3752\n",
      "           8       0.97      0.96      0.97      1316\n",
      "           9       0.95      0.95      0.95       903\n",
      "          10       0.89      0.89      0.89      3589\n",
      "          11       0.93      0.94      0.93      1425\n",
      "          12       0.90      0.84      0.87      1318\n",
      "          13       0.93      0.88      0.90       323\n",
      "          14       0.98      0.97      0.97       226\n",
      "          15       0.98      0.94      0.96       320\n",
      "\n",
      "    accuracy                           0.94     36401\n",
      "   macro avg       0.95      0.94      0.94     36401\n",
      "weighted avg       0.94      0.94      0.94     36401\n",
      "\n",
      "\n",
      "Model Performance Comparison:\n",
      "Model                Accuracy   Log Loss  \n",
      "----------------------------------------\n",
      "LinearSVC            0.9439     0.2128\n",
      "XGBoost              0.9283     0.2413\n",
      "Neural Network       0.9323     0.2617\n",
      "Stacking Ensemble    0.9431     0.2821\n"
     ]
    }
   ],
   "source": [
    "# Split the training data for stacking\n",
    "print(\"Splitting training data for stacking...\")\n",
    "X_train_stack, X_val_stack, y_train_stack, y_val_stack = train_test_split(\n",
    "    X_combined_train_selected, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "X_train_stack_svm, X_val_stack_svm, y_train_stack_svm, y_val_stack_svm = train_test_split(\n",
    "    X_combined_train_selected_svm, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Generate predictions from base models on validation set\n",
    "print(\"Generating base model predictions for stacking...\")\n",
    "val_proba_svm = svm_model.predict_proba(X_val_stack_svm)\n",
    "val_proba_xgb = xgb_model.predict_proba(X_val_stack)\n",
    "val_proba_nn = nn_model.predict(X_val_stack)\n",
    "\n",
    "# Combine predictions for meta-model training\n",
    "print(\"Preparing meta-features for stacking...\")\n",
    "meta_features_val = np.hstack([val_proba_svm, val_proba_xgb, val_proba_nn])\n",
    "\n",
    "# Generate predictions from base models on test set\n",
    "print(\"Generating base model predictions on test set...\")\n",
    "test_proba_svm = svm_model.predict_proba(X_combined_test_selected_svm)\n",
    "test_proba_xgb = xgb_model.predict_proba(X_combined_test_selected)\n",
    "test_proba_nn = nn_model.predict(X_combined_test_selected)\n",
    "\n",
    "# Combine predictions for meta-model testing\n",
    "print(\"Preparing meta-features for test set...\")\n",
    "meta_features_test = np.hstack([test_proba_svm, test_proba_xgb, test_proba_nn])\n",
    "\n",
    "# Train a meta-model (LogisticRegression) on the stacked predictions\n",
    "print(\"Training meta-model for stacking...\")\n",
    "num_classes = len(np.unique(y_train))\n",
    "meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "meta_model.fit(meta_features_val, y_val_stack)\n",
    "\n",
    "# Make predictions with the stacking ensemble\n",
    "print(\"Making predictions with stacking ensemble...\")\n",
    "y_pred_stack = meta_model.predict(meta_features_test)\n",
    "y_proba_stack = meta_model.predict_proba(meta_features_test)\n",
    "\n",
    "# Evaluate stacking ensemble\n",
    "stack_accuracy = accuracy_score(y_test, y_pred_stack)\n",
    "stack_log_loss = multiclass_log_loss(y_test, y_proba_stack)\n",
    "\n",
    "print(f\"Stacking Ensemble Accuracy: {stack_accuracy:.4f}\")\n",
    "print(f\"Stacking Ensemble Log Loss: {stack_log_loss:.4f}\")\n",
    "print(\"\\nStacking Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Log Loss':<10}\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"{'LinearSVC':<20} {svm_accuracy:.4f}     {svm_log_loss:.4f}\")\n",
    "print(f\"{'XGBoost':<20} {xgb_accuracy:.4f}     {xgb_log_loss:.4f}\")\n",
    "print(f\"{'Neural Network':<20} {nn_accuracy:.4f}     {nn_log_loss:.4f}\")\n",
    "print(f\"{'Stacking Ensemble':<20} {stack_accuracy:.4f}     {stack_log_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87d70b9bbf401d",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions for Test Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dbc26f9e3030a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for test products\n",
    "print(\"\\nCreating features for test products...\")\n",
    "\n",
    "# 1. Create graph features for test products\n",
    "print(\"\\nExtracting graph features for test products...\")\n",
    "test_graph_features_final = extract_graph_features(G, test_products)\n",
    "test_graph_features_final = test_graph_features_final.fillna(0)\n",
    "test_graph_features_final_scaled = graph_scaler.transform(test_graph_features_final)\n",
    "test_graph_sparse_final = csr_matrix(test_graph_features_final_scaled)\n",
    "print(f\"Graph features shape for test products: {test_graph_sparse_final.shape}\")\n",
    "\n",
    "# 2. Create Node2Vec embeddings for test products\n",
    "test_node2vec_features_final = np.zeros((len(test_products), 128))\n",
    "for i, node_id in enumerate(test_products):\n",
    "    try:\n",
    "        test_node2vec_features_final[i] = n2v_model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in embeddings, use zeros\n",
    "        pass\n",
    "\n",
    "# Scale the Node2Vec embeddings\n",
    "test_node2vec_scaled_final = n2v_scaler.transform(test_node2vec_features_final)\n",
    "test_node2vec_sparse_final = csr_matrix(test_node2vec_scaled_final)\n",
    "print(f\"Node2Vec features shape for test products: {test_node2vec_sparse_final.shape}\")\n",
    "\n",
    "# 3. Create price features for test products\n",
    "test_price_features_final = create_price_features(test_products, price_df)\n",
    "test_price_scaled_final = test_price_features_final.copy()\n",
    "test_price_scaled_final[price_columns_to_scale] = price_scaler.transform(test_price_features_final[price_columns_to_scale])\n",
    "test_price_features_array_final = test_price_scaled_final.values\n",
    "test_price_sparse_final = csr_matrix(test_price_features_array_final)\n",
    "print(f\"Price features shape for test products: {test_price_sparse_final.shape}\")\n",
    "\n",
    "# COMBINE ALL FEATURES\n",
    "print(\"\\nCombining all features...\")\n",
    "X_combined_test_final = hstack([\n",
    "    X_tfidf_test_comp, \n",
    "    test_graph_sparse_final,\n",
    "    test_node2vec_sparse_final, \n",
    "    test_price_sparse_final\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Combined features shape for test products: {X_combined_test_final.shape}\")\n",
    "\n",
    "# Apply feature selection\n",
    "X_combined_test_final_selected = feature_selector.transform((X_combined_test_final))\n",
    "X_combined_test_final_selected_svm = feature_selector_svm.transform((X_combined_test_final))\n",
    "\n",
    "# Make predictions using all base models\n",
    "print(\"\\nMaking predictions using all base models...\")\n",
    "test_pred_proba_svm = svm_model.predict_proba(X_combined_test_final_selected_svm)\n",
    "test_pred_proba_xgb = xgb_model.predict_proba(X_combined_test_final_selected)\n",
    "test_pred_proba_nn = nn_model.predict(X_combined_test_final_selected)\n",
    "\n",
    "# Combine predictions for meta-model\n",
    "print(\"Preparing meta-features for final predictions...\")\n",
    "meta_features_final = np.hstack([test_pred_proba_svm, test_pred_proba_xgb, test_pred_proba_nn])\n",
    "\n",
    "# Make predictions with the stacking ensemble\n",
    "print(\"Making final predictions with stacking ensemble...\")\n",
    "test_pred_proba_stack = meta_model.predict_proba(meta_features_final)\n",
    "\n",
    "print(f\"Predictions shape: {test_pred_proba_stack.shape}\")\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['product'] = test_products\n",
    "\n",
    "# Add probability for each class\n",
    "for i in range(test_pred_proba_stack.shape[1]):\n",
    "    predictions_df[f'class{i}'] = test_pred_proba_stack[:, i].round(4)\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('predictions_stacking.csv', index=False)\n",
    "print(f\"Predictions saved to predictions_stacking.csv\")\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(\"\\nSample of predictions:\")\n",
    "print(predictions_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
