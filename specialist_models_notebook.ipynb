{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270a98252221557f",
   "metadata": {},
   "source": [
    "# Product Classification using TF-IDF, Graph Features, Ensemble Models, and Specialist Models\n",
    "\n",
    "This notebook extends the implementation from `final_notebook.ipynb` by adding specialist models for classes 1, 10, and 12.\n",
    "\n",
    "Key additions:\n",
    "1. Training specialist models using linear SVC for classes 1, 10, and 12\n",
    "2. Modifying the prediction logic to use these specialist models when:\n",
    "   - The ensemble model predicts class 1, 10, or 12\n",
    "   - The prediction probability is less than 0.9\n",
    "3. If the specialist confirms the class, bump probability to 0.9\n",
    "4. If the specialist disagrees, swap the two highest probability scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daac8a59256dbcd",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6fa784fdbba7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 17:21:30.474556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748614890.536017   85957 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748614890.553331   85957 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748614890.687418   85957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748614890.687441   85957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748614890.687444   85957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748614890.687447   85957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-30 17:21:30.704004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Graph embeddings\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bb9f751c53778",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef54340e1d64d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create price features (from tfidf_node2vec_classification.ipynb)\n",
    "def create_price_features(product_ids, price_df):\n",
    "    \"\"\"\n",
    "    Create price-based features for a list of product IDs\n",
    "\n",
    "    Args:\n",
    "        product_ids: List of product IDs\n",
    "        price_df: DataFrame with product_id and price columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with price features\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with product IDs as index\n",
    "    price_features = pd.DataFrame(index=product_ids)\n",
    "\n",
    "    # Map prices to products\n",
    "    price_dict = dict(zip(price_df['product_id'], price_df['price']))\n",
    "    price_features['price'] = price_features.index.map(lambda x: price_dict.get(x, np.nan))\n",
    "\n",
    "    # Fill missing prices with median\n",
    "    median_price = price_df['price'].median()\n",
    "    price_features['price'].fillna(median_price, inplace=True)\n",
    "\n",
    "    # Create price buckets (as binary features)\n",
    "    price_features['price_0_10'] = (price_features['price'] <= 10).astype(int)\n",
    "    price_features['price_10_100'] = ((price_features['price'] > 10) & (price_features['price'] <= 100)).astype(int)\n",
    "    price_features['price_100_plus'] = (price_features['price'] > 100).astype(int)\n",
    "\n",
    "    # Log transformation of price\n",
    "    price_features['price_log'] = np.log1p(price_features['price'])\n",
    "\n",
    "    # Price rank (percentile)\n",
    "    price_features['price_rank'] = price_features['price'].rank(pct=True)\n",
    "\n",
    "    # Z-score of price (how many standard deviations from the mean)\n",
    "    mean_price = price_df['price'].mean()\n",
    "    std_price = price_df['price'].std()\n",
    "    price_features['price_zscore'] = (price_features['price'] - mean_price) / std_price\n",
    "\n",
    "    return price_features\n",
    "\n",
    "# Function to extract graph features for a set of nodes (from tfidf_node2vec_classification.ipynb)\n",
    "def extract_graph_features(G, node_list):\n",
    "    print(\"Calculating degree centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    print(\"Calculating clustering coefficient...\")\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "    print(\"Calculating PageRank...\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "\n",
    "    print(\"Calculating triangle count...\")\n",
    "    triangles = nx.triangles(G)\n",
    "\n",
    "    # Create a dataframe with the features\n",
    "    features_df = pd.DataFrame(index=node_list)\n",
    "\n",
    "    features_df['degree_centrality'] = features_df.index.map(lambda x: degree_centrality.get(str(x), 0))\n",
    "    features_df['clustering_coefficient'] = features_df.index.map(lambda x: clustering_coefficient.get(str(x), 0))\n",
    "    features_df['pagerank'] = features_df.index.map(lambda x: pagerank.get(str(x), 0))\n",
    "    features_df['triangle_count'] = features_df.index.map(lambda x: triangles.get(str(x), 0))\n",
    "\n",
    "    # Degree (number of connections)\n",
    "    print(\"Calculating degree...\")\n",
    "    degree_dict = dict(G.degree())\n",
    "    features_df['degree'] = features_df.index.map(lambda x: degree_dict.get(str(x), 0))\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Text preprocessing function with lemmatization (from Data_Challenge_TFIDF.ipynb)\n",
    "def clean_text_with_lemma(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Use spaCy to tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if token.lemma_.lower() not in STOP_WORDS\n",
    "        and not token.is_punct\n",
    "        and not token.is_space\n",
    "        and not token.like_num\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to calculate multiclass log loss (from both notebooks)\n",
    "def multiclass_log_loss(y_true, y_pred_proba, eps=1e-15):\n",
    "    \"\"\"\n",
    "    y_true: array-like of shape (N,) - true class labels\n",
    "    y_pred_proba: array-like of shape (N, C) - predicted class probabilities\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # One-hot encode the true labels (yij)\n",
    "    y_true_one_hot = label_binarize(y_true, classes=np.arange(y_pred_proba.shape[1]))\n",
    "\n",
    "    # Clip predicted probabilities to avoid log(0)\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    # Compute the log loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred_proba)) / N\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53429e4a7d8931dd",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300ce3d4d72c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list shape: (1811087, 2)\n",
      "Labels shape: (182006, 2)\n",
      "Train set shape: (145604, 3)\n",
      "Test set shape: (36402, 3)\n",
      "Price data shape: (198817, 2)\n",
      "\n",
      "Edge list sample:\n",
      "   source  target\n",
      "0  251528  237411\n",
      "1  100805   74791\n",
      "2   38634   97747\n",
      "3  247470   77089\n",
      "4  267060  250490\n",
      "\n",
      "Labels sample:\n",
      "   product_id  label\n",
      "0       66795      9\n",
      "1      242781      3\n",
      "2       91280      2\n",
      "3       56356      5\n",
      "4      218494      0\n",
      "\n",
      "Train set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0      114704  hornady unprimed winchester cartridge case hor...      2\n",
      "1      250731  tachikara tk leopard knee pad tachikara tk leo...     11\n",
      "2      152967  g asd replacement cutter aluminum amp carbon u...      2\n",
      "3        4541  mtech usa mt tactical folding knife inch close...      2\n",
      "4      142062  nhl pittsburgh penguins game day black pro sha...      7\n",
      "\n",
      "Test set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0       56218                             katz hoodie volleyball      0\n",
      "1       42346  vz grip operator ii standard size gun grip usa...      2\n",
      "2      215842  tough flat leather hobble tough flat leather h...     15\n",
      "3       36062  buzzrack skipper bicycle suv hatchback wheel c...      1\n",
      "4      188250  cuisinart cgg allfood btu portable outdoor tab...     10\n",
      "\n",
      "Price data sample:\n",
      "   product_id   price\n",
      "0           0   50.50\n",
      "1           2   17.97\n",
      "2           3    4.99\n",
      "3           4   36.60\n",
      "4           5  199.95\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model for text preprocessing\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the edge list data\n",
    "edgelist_file = 'data_files/edgelist.txt'\n",
    "edges_df = pd.read_csv(edgelist_file, header=None, names=['source', 'target'])\n",
    "\n",
    "# Load the class labels\n",
    "labels_file = 'y_train.txt'\n",
    "labels_df = pd.read_csv(labels_file, header=None, names=['product_id', 'label'])\n",
    "\n",
    "# Load the train and test splits\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Load price data\n",
    "price_df = pd.read_csv('data_files/price.txt', header=None, names=['product_id', 'price'])\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Edge list shape: {edges_df.shape}\")\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Price data shape: {price_df.shape}\")\n",
    "\n",
    "# Check the first few rows of each dataset\n",
    "print(\"\\nEdge list sample:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nLabels sample:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "print(\"\\nTrain set sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(price_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710953a6ae0da25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'description' in train_df:\n",
      "       product_id text_clean  label\n",
      "89285      265165        NaN      7\n",
      "\n",
      "Rows with missing 'description' in test_df:\n",
      "       product_id text_clean  label\n",
      "34767      174103        NaN      5\n",
      "Missing values in cleaned train_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Missing values in cleaned test_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Loading product IDs from test.txt...\n",
      "Loaded 45502 product IDs from test.txt\n",
      "First 10 product IDs: ['49957', '135386', '226880', '165114', '256154', '254193', '20830', '46170', '19248', '158023']\n",
      "Getting descriptions\n",
      "Apply cleaning\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in text data\n",
    "# Find rows with missing 'description' in train_df\n",
    "missing_train = train_df[train_df['text_clean'].isnull()]\n",
    "\n",
    "# Find rows with missing 'description' in test_df\n",
    "missing_test = test_df[test_df['text_clean'].isnull()]\n",
    "\n",
    "# Print the rows with missing values\n",
    "print(\"Rows with missing 'description' in train_df:\")\n",
    "print(missing_train)\n",
    "\n",
    "print(\"\\nRows with missing 'description' in test_df:\")\n",
    "print(missing_test)\n",
    "\n",
    "# Remove rows with missing 'description' in train_df\n",
    "train_df = train_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Remove rows with missing 'description' in test_df\n",
    "test_df = test_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Verify if any rows with missing values remain\n",
    "print(\"Missing values in cleaned train_df:\", train_df.isnull().sum())\n",
    "print(\"Missing values in cleaned test_df:\", test_df.isnull().sum())\n",
    "\n",
    "# Reset the index after removing rows with missing values\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Load product IDs from test.txt for final predictions\n",
    "print(\"Loading product IDs from test.txt...\")\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_products = [line.strip().rstrip(',') for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(test_products)} product IDs from test.txt\")\n",
    "print(\"First 10 product IDs:\", test_products[:10])\n",
    "\n",
    "# Load descriptions for all products\n",
    "print(\"Getting descriptions\")\n",
    "descriptions = dict()\n",
    "with open(\"data_files/description_part_1.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "with open(\"data_files/description_part_2.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "# Get descriptions for test products\n",
    "test_text = []\n",
    "for i in test_products:\n",
    "    try:\n",
    "        test_text.append(descriptions[int(i)])\n",
    "    except (KeyError, ValueError):\n",
    "        test_text.append(\"\")  # Empty string for missing descriptions\n",
    "\n",
    "# Apply the cleaning function to test data\n",
    "print(\"Apply cleaning\")\n",
    "test_text_cleaned = [clean_text_with_lemma(text) for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14af450496ad180",
   "metadata": {},
   "source": [
    "## 4. Creating Graph and Extracting Graph Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568eac9ad1f7a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Number of nodes: 276453\n",
      "Number of edges: 1811087\n",
      "Network Density: 0.000047\n",
      "Is the graph connected? False\n",
      "\n",
      "Largest Connected Component:\n",
      "  Nodes: 273012\n",
      "  Edges: 1808230\n",
      "  Percentage of total nodes: 98.76%\n",
      "\n",
      "Extracting graph features for training set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "\n",
      "Extracting graph features for testing set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "Loading Node2Vec embeddings...\n",
      "Loading existing Node2Vec model from disk...\n",
      "Node2Vec embeddings shape - Train: (145603, 128), Test: (36401, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create a graph from the edge list\n",
    "print(\"Creating graph\")\n",
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f\"Is the graph connected? {is_connected}\")\n",
    "\n",
    "if not is_connected:\n",
    "    # Get the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    print(f\"\\nLargest Connected Component:\")\n",
    "    print(f\"  Nodes: {largest_cc_subgraph.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {largest_cc_subgraph.number_of_edges()}\")\n",
    "    print(f\"  Percentage of total nodes: {largest_cc_subgraph.number_of_nodes() / G.number_of_nodes() * 100:.2f}%\")\n",
    "\n",
    "# Get the list of product IDs from train and test sets\n",
    "train_product_ids = train_df['product_id'].tolist()\n",
    "test_product_ids = test_df['product_id'].tolist()\n",
    "\n",
    "# Extract graph features for training and testing sets\n",
    "print(\"\\nExtracting graph features for training set...\")\n",
    "train_graph_features = extract_graph_features(G, train_product_ids)\n",
    "\n",
    "print(\"\\nExtracting graph features for testing set...\")\n",
    "test_graph_features = extract_graph_features(G, test_product_ids)\n",
    "\n",
    "# Fill missing values with 0 if any\n",
    "train_graph_features = train_graph_features.fillna(0)\n",
    "test_graph_features = test_graph_features.fillna(0)\n",
    "\n",
    "# Scale the features\n",
    "graph_scaler = StandardScaler()\n",
    "train_graph_features_scaled = graph_scaler.fit_transform(train_graph_features)\n",
    "test_graph_features_scaled = graph_scaler.transform(test_graph_features)\n",
    "\n",
    "# Generate Node2Vec embeddings\n",
    "model_path = \"node2vec.model\"\n",
    "print(\"Loading Node2Vec embeddings...\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing Node2Vec model from disk...\")\n",
    "        n2v_model = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        # Convert NetworkX graph to node2vec format\n",
    "        # First, ensure all nodes are strings for compatibility\n",
    "        G_node2vec = nx.Graph()\n",
    "        for edge in G.edges():\n",
    "            G_node2vec.add_edge(str(edge[0]), str(edge[1]))\n",
    "\n",
    "        # Initialize node2vec model\n",
    "        node2vec_model = node2vec.Node2Vec(\n",
    "            G_node2vec,\n",
    "            dimensions=128,  # Embedding dimension\n",
    "            walk_length=10,  # Length of each random walk\n",
    "            num_walks=10,    # Number of random walks per node\n",
    "            workers=1       # Number of parallel workers\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training Node2Vec model...\")\n",
    "        n2v_model = node2vec_model.fit(\n",
    "            window=10,       # Context size for optimization\n",
    "            min_count=1,     # Minimum count of node occurrences\n",
    "            batch_words=4    # Number of words per batch\n",
    "        )\n",
    "\n",
    "        n2v_model.save(model_path)\n",
    "\n",
    "    # Generate embeddings for train and test nodes\n",
    "    train_node2vec_features = np.zeros((len(train_product_ids), 128))\n",
    "    test_node2vec_features = np.zeros((len(test_product_ids), 128))\n",
    "\n",
    "    # Extract embeddings for training nodes\n",
    "    for i, node_id in enumerate(train_product_ids):\n",
    "        try:\n",
    "            train_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    # Extract embeddings for testing nodes\n",
    "    for i, node_id in enumerate(test_product_ids):\n",
    "        try:\n",
    "            test_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    print(f\"Node2Vec embeddings shape - Train: {train_node2vec_features.shape}, Test: {test_node2vec_features.shape}\")\n",
    "\n",
    "    # Scale the embeddings\n",
    "    n2v_scaler = StandardScaler()\n",
    "    train_node2vec_scaled = n2v_scaler.fit_transform(train_node2vec_features)\n",
    "    test_node2vec_scaled = n2v_scaler.transform(test_node2vec_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Node2Vec embeddings: {e}\")\n",
    "    print(\"Skipping Node2Vec embeddings...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd602918f9082848",
   "metadata": {},
   "source": [
    "## 5. Extracting TF-IDF Features from Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d50559199f201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TF-IDF\n",
      "TF-IDF features shape - Train: (145603, 2060013), Test: (36401, 2060013)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing TF-IDF\")\n",
    "# Initialize the TfidfVectorizer with parameters from Data_Challenge_TFIDF.ipynb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95, sublinear_tf=True, norm='l2')\n",
    "\n",
    "# Fit and transform the text data to get the TF-IDF matrix\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "X_tfidf_test_comp = tfidf_vectorizer.transform(test_text_cleaned)  # For final predictions\n",
    "\n",
    "print(f\"TF-IDF features shape - Train: {X_tfidf_train.shape}, Test: {X_tfidf_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c0c7f3bf72f59",
   "metadata": {},
   "source": [
    "## 6. Creating Price Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b15d1d84f8d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating price features for training set...\n",
      "Creating price features for testing set...\n",
      "\n",
      "Train price features sample:\n",
      "        price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "114704  43.20           0             1               0   3.788725   \n",
      "250731  24.99           0             1               0   3.257712   \n",
      "152967  22.95           0             1               0   3.175968   \n",
      "4541     8.49           1             0               0   2.250239   \n",
      "142062  24.99           0             1               0   3.257712   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "114704    0.760266     -0.130318  \n",
      "250731    0.495769     -0.326493  \n",
      "152967    0.339856     -0.348470  \n",
      "4541      0.095352     -0.504247  \n",
      "142062    0.495769     -0.326493  \n",
      "\n",
      "Test price features sample:\n",
      "         price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "56218    11.99           0             1               0   2.564180   \n",
      "42346    65.00           0             1               0   4.189655   \n",
      "215842   23.95           0             1               0   3.216874   \n",
      "36062   119.99           0             0               1   4.795708   \n",
      "188250  159.00           0             0               1   5.075174   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "56218     0.165820     -0.466542  \n",
      "42346     0.832875      0.104533  \n",
      "215842    0.347504     -0.337697  \n",
      "36062     0.912695      0.696937  \n",
      "188250    0.940469      1.117190  \n",
      "Price features shape - Train: (145603, 7), Test: (36401, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create price features for training and testing sets\n",
    "print(\"Creating price features for training set...\")\n",
    "train_price_features = create_price_features(train_product_ids, price_df)\n",
    "\n",
    "print(\"Creating price features for testing set...\")\n",
    "test_price_features = create_price_features(test_product_ids, price_df)\n",
    "\n",
    "# Display the first few rows of price features\n",
    "print(\"\\nTrain price features sample:\")\n",
    "print(train_price_features.head())\n",
    "\n",
    "print(\"\\nTest price features sample:\")\n",
    "print(test_price_features.head())\n",
    "\n",
    "# Scale the price features (except binary features)\n",
    "price_scaler = StandardScaler()\n",
    "price_columns_to_scale = ['price', 'price_log', 'price_rank', 'price_zscore']\n",
    "binary_columns = ['price_0_10', 'price_10_100', 'price_100_plus']\n",
    "\n",
    "# Scale the selected columns\n",
    "train_price_scaled = train_price_features.copy()\n",
    "test_price_scaled = test_price_features.copy()\n",
    "\n",
    "train_price_scaled[price_columns_to_scale] = price_scaler.fit_transform(train_price_features[price_columns_to_scale])\n",
    "test_price_scaled[price_columns_to_scale] = price_scaler.transform(test_price_features[price_columns_to_scale])\n",
    "\n",
    "# Convert to numpy arrays for easier handling\n",
    "train_price_features_array = train_price_scaled.values\n",
    "test_price_features_array = test_price_scaled.values\n",
    "\n",
    "print(f\"Price features shape - Train: {train_price_features_array.shape}, Test: {test_price_features_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b01250d199b20",
   "metadata": {},
   "source": [
    "## 7. Combining Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f58fd9989ef756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting node2vec embeddings to sparse format...\n",
      "Converting graph features to sparse format...\n",
      "Converting price features to sparse format...\n",
      "Combining all features...\n",
      "Final combined features shape - Train: (145603, 2060153), Test: (36401, 2060153)\n",
      "Memory usage (approximate):\n",
      "  - X_combined_train: 230.32 MB\n",
      "  - X_combined_test: 55.17 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert node2vec embeddings to sparse for efficient concatenation\n",
    "print(\"Converting node2vec embeddings to sparse format...\")\n",
    "train_node2vec_sparse = csr_matrix(train_node2vec_scaled)\n",
    "test_node2vec_sparse = csr_matrix(test_node2vec_scaled)\n",
    "\n",
    "# Convert graph features to sparse\n",
    "print(\"Converting graph features to sparse format...\")\n",
    "train_graph_sparse = csr_matrix(train_graph_features_scaled)\n",
    "test_graph_sparse = csr_matrix(test_graph_features_scaled)\n",
    "\n",
    "# Convert price features to sparse\n",
    "print(\"Converting price features to sparse format...\")\n",
    "train_price_sparse = csr_matrix(train_price_features_array)\n",
    "test_price_sparse = csr_matrix(test_price_features_array)\n",
    "\n",
    "# Combine all features: TF-IDF + graph features + node2vec embeddings + price features\n",
    "print(\"Combining all features...\")\n",
    "X_combined_train = hstack([X_tfidf_train, train_graph_sparse, train_node2vec_sparse, train_price_sparse], format='csr')\n",
    "X_combined_test = hstack([X_tfidf_test, test_graph_sparse, test_node2vec_sparse, test_price_sparse], format='csr')\n",
    "\n",
    "# Output memory usage information\n",
    "print(f\"Final combined features shape - Train: {X_combined_train.shape}, Test: {X_combined_test.shape}\")\n",
    "print(\"Memory usage (approximate):\")\n",
    "print(f\"  - X_combined_train: {X_combined_train.data.nbytes / (1024 ** 2):.2f} MB\")\n",
    "print(f\"  - X_combined_test: {X_combined_test.data.nbytes / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a9a63ff704382",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbe84a32afc6805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1000000 features\n",
      "(145603, 1000000)\n",
      "Selecting 2000000 features\n",
      "(145603, 2000000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "k_feat = 1000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected = feature_selector.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected = feature_selector.transform(X_combined_test)\n",
    "print(X_combined_train_selected.shape)\n",
    "\n",
    "# We trained the LinearSVC on 2m features so we use another feature selector\n",
    "k_feat = 2000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector_svm = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected_svm = feature_selector_svm.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected_svm = feature_selector_svm.transform(X_combined_test)\n",
    "print(X_combined_train_selected_svm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a66cb7ef1266f",
   "metadata": {},
   "source": [
    "## 9. Loading Pre-trained Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfcbf6c10cb99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained models...\n",
      "Loaded LinearSVC model\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "print(\"Loading pre-trained models...\")\n",
    "\n",
    "# Load Calibrated LinearSVC model\n",
    "svm_model = joblib.load('linear_svc_model.pkl')\n",
    "print(\"Loaded LinearSVC model\")\n",
    "\n",
    "# Make predictions with each model\n",
    "y_pred_svm = svm_model.predict(X_combined_test_selected_svm)\n",
    "y_proba_svm = svm_model.predict_proba(X_combined_test_selected_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32c4dfa-e1e7-4124-b57a-005a9dc964b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGBoost model\n"
     ]
    }
   ],
   "source": [
    "# Load XGBoost model\n",
    "xgb_model = joblib.load('xgb_model.pkl')\n",
    "print(\"Loaded XGBoost model\")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_combined_test_selected)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_combined_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7287f763-95a4-4520-bb97-3ad00a697e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 17:31:58.930068: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-05-30 17:31:58.959986: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 512000000 exceeds 10% of free system memory.\n",
      "2025-05-30 17:31:59.047115: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 512000000 exceeds 10% of free system memory.\n",
      "2025-05-30 17:31:59.141457: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 512000000 exceeds 10% of free system memory.\n",
      "2025-05-30 17:31:59.929306: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 512000000 exceeds 10% of free system memory.\n",
      "2025-05-30 17:32:00.004333: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 512000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Neural Network model\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 140ms/step\n",
      "\u001b[1m1138/1138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 139ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load Neural Network model\n",
    "nn_model = load_model('neural_network_model.keras')\n",
    "print(\"Loaded Neural Network model\")\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_combined_test_selected)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_combined_test_selected)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ce6572-bdb4-4d38-90bc-eec8cb885984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble weights: SVM=0.60, XGB=0.20, NN=0.20\n",
      "Ensemble Accuracy: 0.9431\n",
      "Ensemble Log Loss: 0.1995\n",
      "\n",
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3033\n",
      "           1       0.91      0.91      0.91      2372\n",
      "           2       0.93      0.95      0.94      8652\n",
      "           3       0.96      0.97      0.96      1073\n",
      "           4       0.95      0.96      0.96      3016\n",
      "           5       0.97      0.97      0.97      3564\n",
      "           6       0.96      0.96      0.96      1519\n",
      "           7       0.96      0.94      0.95      3752\n",
      "           8       0.97      0.97      0.97      1316\n",
      "           9       0.95      0.95      0.95       903\n",
      "          10       0.89      0.89      0.89      3589\n",
      "          11       0.94      0.93      0.93      1425\n",
      "          12       0.89      0.84      0.87      1318\n",
      "          13       0.92      0.87      0.90       323\n",
      "          14       0.97      0.97      0.97       226\n",
      "          15       0.98      0.94      0.96       320\n",
      "\n",
      "    accuracy                           0.94     36401\n",
      "   macro avg       0.95      0.94      0.94     36401\n",
      "weighted avg       0.94      0.94      0.94     36401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define ensemble weights (from final_notebook.ipynb)\n",
    "weights = [0.6, 0.2, 0.2]  # SVM, XGB, NN\n",
    "print(f\"Ensemble weights: SVM={weights[0]:.2f}, XGB={weights[1]:.2f}, NN={weights[2]:.2f}\")\n",
    "\n",
    "# Combine predictions with weighted average\n",
    "y_proba_ensemble = (\n",
    "    weights[0] * y_proba_svm + \n",
    "    weights[1] * y_proba_xgb + \n",
    "    weights[2] * y_pred_nn\n",
    ")\n",
    "\n",
    "# Get class predictions\n",
    "y_pred_ensemble = np.argmax(y_proba_ensemble, axis=1)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_log_loss = multiclass_log_loss(y_test, y_proba_ensemble)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble Log Loss: {ensemble_log_loss:.4f}\")\n",
    "print(\"\\nEnsemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8996264bc97b0",
   "metadata": {},
   "source": [
    "## 10. Training Specialist Models for Classes 1, 10, 12 and 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79d3edad4be78ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training specialist models for classes: [1, 10, 12, 13]\n",
      "\n",
      "Training specialist model for class 1...\n",
      "Saved specialist model to specialist_model_class_1.pkl\n",
      "Specialist model for class 1 - Accuracy: 0.9877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     34029\n",
      "           1       0.89      0.92      0.91      2372\n",
      "\n",
      "    accuracy                           0.99     36401\n",
      "   macro avg       0.94      0.96      0.95     36401\n",
      "weighted avg       0.99      0.99      0.99     36401\n",
      "\n",
      "\n",
      "Training specialist model for class 10...\n",
      "Saved specialist model to specialist_model_class_10.pkl\n",
      "Specialist model for class 10 - Accuracy: 0.9776\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     32812\n",
      "           1       0.88      0.89      0.89      3589\n",
      "\n",
      "    accuracy                           0.98     36401\n",
      "   macro avg       0.94      0.94      0.94     36401\n",
      "weighted avg       0.98      0.98      0.98     36401\n",
      "\n",
      "\n",
      "Training specialist model for class 12...\n",
      "Saved specialist model to specialist_model_class_12.pkl\n",
      "Specialist model for class 12 - Accuracy: 0.9903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     35083\n",
      "           1       0.88      0.85      0.86      1318\n",
      "\n",
      "    accuracy                           0.99     36401\n",
      "   macro avg       0.94      0.92      0.93     36401\n",
      "weighted avg       0.99      0.99      0.99     36401\n",
      "\n",
      "\n",
      "Training specialist model for class 13...\n",
      "Saved specialist model to specialist_model_class_13.pkl\n",
      "Specialist model for class 13 - Accuracy: 0.9982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     36078\n",
      "           1       0.92      0.87      0.90       323\n",
      "\n",
      "    accuracy                           1.00     36401\n",
      "   macro avg       0.96      0.93      0.95     36401\n",
      "weighted avg       1.00      1.00      1.00     36401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the specialist classes\n",
    "specialist_classes = [1, 10, 12, 13]\n",
    "print(f\"Training specialist models for classes: {specialist_classes}\")\n",
    "\n",
    "# Create binary classification datasets for each specialist class\n",
    "specialist_models = {}\n",
    "\n",
    "for specialist_class in specialist_classes:\n",
    "    print(f\"\\nTraining specialist model for class {specialist_class}...\")\n",
    "    \n",
    "    # Create binary labels (1 for the specialist class, 0 for others)\n",
    "    y_train_binary = (y_train == specialist_class).astype(int)\n",
    "    \n",
    "    # Train a LinearSVC model\n",
    "    base_svc = LinearSVC(max_iter=10000, dual=False, class_weight={0: 1, 1: 20})\n",
    "    specialist_model = CalibratedClassifierCV(base_svc, cv=3, method='isotonic')\n",
    "    \n",
    "    # Train on the selected features\n",
    "    specialist_model.fit(X_combined_train_selected_svm, y_train_binary)\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f'specialist_model_class_{specialist_class}.pkl'\n",
    "    joblib.dump(specialist_model, model_filename)\n",
    "    print(f\"Saved specialist model to {model_filename}\")\n",
    "    \n",
    "    # Store the model in the dictionary\n",
    "    specialist_models[specialist_class] = specialist_model\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_test_binary = (y_test == specialist_class).astype(int)\n",
    "    y_pred_binary = specialist_model.predict(X_combined_test_selected_svm)\n",
    "    y_proba_binary = specialist_model.predict_proba(X_combined_test_selected_svm)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    \n",
    "    print(f\"Specialist model for class {specialist_class} - Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test_binary, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21250c15e44edd",
   "metadata": {},
   "source": [
    "## 11. Modified Prediction Logic with Specialist Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7795a7eb10545321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying specialist model logic to ensemble predictions...\n",
      "Modified Ensemble Accuracy: 0.9423\n",
      "Modified Ensemble Log Loss: 0.2071\n",
      "\n",
      "Modified Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      3033\n",
      "           1       0.92      0.90      0.91      2372\n",
      "           2       0.93      0.96      0.94      8652\n",
      "           3       0.96      0.97      0.96      1073\n",
      "           4       0.95      0.96      0.96      3016\n",
      "           5       0.97      0.97      0.97      3564\n",
      "           6       0.96      0.96      0.96      1519\n",
      "           7       0.96      0.94      0.95      3752\n",
      "           8       0.97      0.97      0.97      1316\n",
      "           9       0.95      0.95      0.95       903\n",
      "          10       0.91      0.87      0.89      3589\n",
      "          11       0.93      0.93      0.93      1425\n",
      "          12       0.90      0.82      0.86      1318\n",
      "          13       0.94      0.86      0.90       323\n",
      "          14       0.97      0.97      0.97       226\n",
      "          15       0.97      0.94      0.96       320\n",
      "\n",
      "    accuracy                           0.94     36401\n",
      "   macro avg       0.95      0.94      0.94     36401\n",
      "weighted avg       0.94      0.94      0.94     36401\n",
      "\n",
      "\n",
      "Comparison of Original vs Modified Ensemble:\n",
      "Original Ensemble Accuracy: 0.9431, Log Loss: 0.1995\n",
      "Modified Ensemble Accuracy: 0.9423, Log Loss: 0.2071\n",
      "Number of predictions changed by specialist models: 284 (0.78%)\n"
     ]
    }
   ],
   "source": [
    "# Function to apply specialist model logic to predictions\n",
    "def apply_specialist_models(ensemble_probs, specialist_models, specialist_classes, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Apply specialist model logic to ensemble predictions\n",
    "    \n",
    "    Args:\n",
    "        ensemble_probs: Array of shape (n_samples, n_classes) with ensemble probabilities\n",
    "        specialist_models: Dictionary mapping class to specialist model\n",
    "        specialist_classes: List of classes that have specialist models\n",
    "        threshold: Probability threshold below which to consult specialist models\n",
    "        \n",
    "    Returns:\n",
    "        Modified probability array\n",
    "    \"\"\"\n",
    "    # Make a copy of the input probabilities to avoid modifying the original\n",
    "    modified_probs = ensemble_probs.copy()\n",
    "    \n",
    "    # For each sample\n",
    "    for i in range(ensemble_probs.shape[0]):\n",
    "        # Get the predicted class and its probability\n",
    "        pred_class = np.argmax(ensemble_probs[i])\n",
    "        pred_prob = ensemble_probs[i, pred_class]\n",
    "        \n",
    "        # Check if the predicted class is one of the specialist classes and probability is below threshold\n",
    "        if pred_class in specialist_classes and pred_prob < threshold:\n",
    "            # Get the specialist model for this class\n",
    "            specialist_model = specialist_models[pred_class]\n",
    "            \n",
    "            # Get the features for this sample (need to extract from the appropriate feature set)\n",
    "            # This depends on which feature set was used to train the specialist model\n",
    "            sample_features = X_combined_test_selected_svm[i]\n",
    "            \n",
    "            # Get the specialist's prediction (probability of being the specialist class)\n",
    "            specialist_prob = specialist_model.predict_proba(sample_features)[0, 1]\n",
    "            \n",
    "            # If the specialist agrees (probability > 0.5)\n",
    "            if specialist_prob > 0.5:\n",
    "                # Bump the probability up to 0.9\n",
    "                old_prob = modified_probs[i, pred_class]\n",
    "                modified_probs[i, pred_class] = threshold\n",
    "                \n",
    "                # Calculate how much probability was added\n",
    "                prob_added = threshold - old_prob\n",
    "                \n",
    "                # Distribute the added probability proportionally from other classes\n",
    "                # to maintain sum = 1\n",
    "                for j in range(ensemble_probs.shape[1]):\n",
    "                    if j != pred_class:\n",
    "                        # Reduce other probabilities proportionally\n",
    "                        modified_probs[i, j] = modified_probs[i, j] * (1 - threshold) / (1 - old_prob)\n",
    "            else:\n",
    "                # Specialist disagrees, swap the two highest scores\n",
    "                # Find the indices of the two highest probabilities\n",
    "                top_indices = np.argsort(ensemble_probs[i])[-2:]\n",
    "                \n",
    "                # Swap the values\n",
    "                temp = modified_probs[i, top_indices[0]]\n",
    "                modified_probs[i, top_indices[0]] = modified_probs[i, top_indices[1]]\n",
    "                modified_probs[i, top_indices[1]] = temp\n",
    "    \n",
    "    return modified_probs\n",
    "\n",
    "# Apply the specialist model logic to the ensemble predictions\n",
    "print(\"Applying specialist model logic to ensemble predictions...\")\n",
    "modified_ensemble_probs = apply_specialist_models(\n",
    "    y_proba_ensemble, \n",
    "    specialist_models, \n",
    "    specialist_classes, \n",
    "    threshold=0.9\n",
    ")\n",
    "\n",
    "# Get class predictions from modified probabilities\n",
    "modified_pred_ensemble = np.argmax(modified_ensemble_probs, axis=1)\n",
    "\n",
    "# Evaluate modified ensemble\n",
    "modified_ensemble_accuracy = accuracy_score(y_test, modified_pred_ensemble)\n",
    "modified_ensemble_log_loss = multiclass_log_loss(y_test, modified_ensemble_probs)\n",
    "\n",
    "print(f\"Modified Ensemble Accuracy: {modified_ensemble_accuracy:.4f}\")\n",
    "print(f\"Modified Ensemble Log Loss: {modified_ensemble_log_loss:.4f}\")\n",
    "print(\"\\nModified Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, modified_pred_ensemble))\n",
    "\n",
    "# Compare original and modified ensemble\n",
    "print(\"\\nComparison of Original vs Modified Ensemble:\")\n",
    "print(f\"Original Ensemble Accuracy: {ensemble_accuracy:.4f}, Log Loss: {ensemble_log_loss:.4f}\")\n",
    "print(f\"Modified Ensemble Accuracy: {modified_ensemble_accuracy:.4f}, Log Loss: {modified_ensemble_log_loss:.4f}\")\n",
    "\n",
    "# Count how many predictions were changed by the specialist models\n",
    "changes = np.sum(y_pred_ensemble != modified_pred_ensemble)\n",
    "print(f\"Number of predictions changed by specialist models: {changes} ({changes/len(y_test)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68485151-d6eb-437a-9e72-0da0eb403340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive threshold optimization...\n",
      "Testing 7 confidence thresholds: [0.6  0.65 0.7  0.75 0.8  0.85 0.9 ]\n",
      "Testing 8 boost probabilities: [0.8  0.82 0.84 0.86 0.88 0.9  0.92 0.94]\n",
      "Total combinations: 56\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Original Ensemble Performance:\n",
      "Accuracy: 0.9431\n",
      "Log Loss: 0.1995\n",
      "\n",
      "Best Accuracy Configuration:\n",
      "Confidence Threshold: 0.60\n",
      "Boost Probability: 0.80\n",
      "Accuracy: 0.9431 (Δ: +0.0000)\n",
      "Log Loss: 0.2009 (Δ: +0.0014)\n",
      "Changes: 205.0 (0.6%)\n",
      "Interventions: 568.0 (1.6%)\n",
      "\n",
      "Best Log Loss Configuration:\n",
      "Confidence Threshold: 0.60\n",
      "Boost Probability: 0.80\n",
      "Accuracy: 0.9431 (Δ: +0.0000)\n",
      "Log Loss: 0.2009 (Δ: +0.0014)\n",
      "Changes: 205.0 (0.6%)\n",
      "Interventions: 568.0 (1.6%)\n",
      "\n",
      "Top 10 Configurations by Accuracy:\n",
      " confidence_threshold  boost_probability  accuracy  log_loss  change_percent\n",
      "               0.6000             0.8000    0.9431    0.2009          0.5632\n",
      "               0.6000             0.8200    0.9431    0.2011          0.5632\n",
      "               0.6000             0.8400    0.9431    0.2015          0.5632\n",
      "               0.6000             0.8600    0.9431    0.2019          0.5632\n",
      "               0.6000             0.8800    0.9431    0.2024          0.5632\n",
      "               0.6000             0.9000    0.9431    0.2030          0.5632\n",
      "               0.6000             0.9200    0.9431    0.2037          0.5632\n",
      "               0.6000             0.9400    0.9431    0.2048          0.5632\n",
      "               0.6500             0.8000    0.9431    0.2011          0.6428\n",
      "               0.6500             0.8200    0.9431    0.2015          0.6428\n",
      "\n",
      "Top 10 Configurations by Log Loss (lowest):\n",
      " confidence_threshold  boost_probability  accuracy  log_loss  change_percent\n",
      "               0.6000             0.8000    0.9431    0.2009          0.5632\n",
      "               0.6000             0.8200    0.9431    0.2011          0.5632\n",
      "               0.6500             0.8000    0.9431    0.2011          0.6428\n",
      "               0.6000             0.8400    0.9431    0.2015          0.5632\n",
      "               0.6500             0.8200    0.9431    0.2015          0.6428\n",
      "               0.7000             0.8000    0.9428    0.2017          0.7033\n",
      "               0.6000             0.8600    0.9431    0.2019          0.5632\n",
      "               0.7500             0.8000    0.9426    0.2019          0.7390\n",
      "               0.6500             0.8400    0.9431    0.2020          0.6428\n",
      "               0.7000             0.8200    0.9428    0.2022          0.7033\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import log_loss as multiclass_log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def apply_specialist_models_flexible(ensemble_probs, specialist_models, specialist_classes, \n",
    "                                   confidence_threshold=0.9, boost_probability=0.9):\n",
    "    \"\"\"\n",
    "    Apply specialist model logic with flexible thresholds\n",
    "    \n",
    "    Args:\n",
    "        ensemble_probs: Array of shape (n_samples, n_classes) with ensemble probabilities\n",
    "        specialist_models: Dictionary mapping class to specialist model\n",
    "        specialist_classes: List of classes that have specialist models\n",
    "        confidence_threshold: Probability threshold below which to consult specialist models\n",
    "        boost_probability: Probability to set when specialist agrees\n",
    "        \n",
    "    Returns:\n",
    "        Modified probability array\n",
    "    \"\"\"\n",
    "    modified_probs = ensemble_probs.copy()\n",
    "    interventions = 0\n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    \n",
    "    for i in range(ensemble_probs.shape[0]):\n",
    "        pred_class = np.argmax(ensemble_probs[i])\n",
    "        pred_prob = ensemble_probs[i, pred_class]\n",
    "        \n",
    "        # Check if predicted class has specialist and probability is below threshold\n",
    "        if pred_class in specialist_classes and pred_prob < confidence_threshold:\n",
    "            interventions += 1\n",
    "            specialist_model = specialist_models[pred_class]\n",
    "            \n",
    "            # Get specialist prediction\n",
    "            sample_features = X_combined_test_selected_svm[i].reshape(1, -1)\n",
    "            specialist_prob = specialist_model.predict_proba(sample_features)[0, 1]\n",
    "            \n",
    "            if specialist_prob > 0.5:\n",
    "                # Specialist agrees - boost probability\n",
    "                agreements += 1\n",
    "                old_prob = modified_probs[i, pred_class]\n",
    "                modified_probs[i, pred_class] = boost_probability\n",
    "                \n",
    "                # Redistribute remaining probability proportionally\n",
    "                remaining_prob = 1 - boost_probability\n",
    "                original_remaining = 1 - old_prob\n",
    "                \n",
    "                if original_remaining > 0:  # Avoid division by zero\n",
    "                    for j in range(ensemble_probs.shape[1]):\n",
    "                        if j != pred_class:\n",
    "                            modified_probs[i, j] = modified_probs[i, j] * remaining_prob / original_remaining\n",
    "            else:\n",
    "                # Specialist disagrees - swap top two\n",
    "                disagreements += 1\n",
    "                top_indices = np.argsort(ensemble_probs[i])[-2:]\n",
    "                temp = modified_probs[i, top_indices[0]]\n",
    "                modified_probs[i, top_indices[0]] = modified_probs[i, top_indices[1]]\n",
    "                modified_probs[i, top_indices[1]] = temp\n",
    "    \n",
    "    return modified_probs, interventions, agreements, disagreements\n",
    "\n",
    "# Define parameter ranges\n",
    "confidence_thresholds = np.arange(0.6, 0.95, 0.05)  # 0.6 to 0.9 in steps of 0.05\n",
    "boost_probabilities = np.arange(0.80, 0.96, 0.02)   # 0.8 to 0.95 in steps of 0.02\n",
    "\n",
    "print(\"Starting comprehensive threshold optimization...\")\n",
    "print(f\"Testing {len(confidence_thresholds)} confidence thresholds: {confidence_thresholds}\")\n",
    "print(f\"Testing {len(boost_probabilities)} boost probabilities: {boost_probabilities}\")\n",
    "print(f\"Total combinations: {len(confidence_thresholds) * len(boost_probabilities)}\")\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Test all combinations\n",
    "for conf_thresh, boost_prob in product(confidence_thresholds, boost_probabilities):\n",
    "    # Apply specialist logic with current parameters\n",
    "    modified_probs, interventions, agreements, disagreements = apply_specialist_models_flexible(\n",
    "        y_proba_ensemble, \n",
    "        specialist_models, \n",
    "        specialist_classes, \n",
    "        confidence_threshold=conf_thresh,\n",
    "        boost_probability=boost_prob\n",
    "    )\n",
    "    \n",
    "    # Get predictions and evaluate\n",
    "    modified_pred = np.argmax(modified_probs, axis=1)\n",
    "    accuracy = accuracy_score(y_test, modified_pred)\n",
    "    log_loss = multiclass_log_loss(y_test, modified_probs)\n",
    "    \n",
    "    # Count changes from original\n",
    "    changes = np.sum(y_pred_ensemble != modified_pred)\n",
    "    change_percent = changes / len(y_test) * 100\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'confidence_threshold': conf_thresh,\n",
    "        'boost_probability': boost_prob,\n",
    "        'accuracy': accuracy,\n",
    "        'log_loss': log_loss,\n",
    "        'interventions': interventions,\n",
    "        'agreements': agreements,\n",
    "        'disagreements': disagreements,\n",
    "        'changes': changes,\n",
    "        'change_percent': change_percent,\n",
    "        'intervention_rate': interventions / len(y_test) * 100,\n",
    "        'agreement_rate': agreements / interventions * 100 if interventions > 0 else 0\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configurations\n",
    "best_accuracy = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_log_loss = results_df.loc[results_df['log_loss'].idxmin()]\n",
    "\n",
    "print(f\"\\nOriginal Ensemble Performance:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Log Loss: {ensemble_log_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Accuracy Configuration:\")\n",
    "print(f\"Confidence Threshold: {best_accuracy['confidence_threshold']:.2f}\")\n",
    "print(f\"Boost Probability: {best_accuracy['boost_probability']:.2f}\")\n",
    "print(f\"Accuracy: {best_accuracy['accuracy']:.4f} (Δ: {best_accuracy['accuracy'] - ensemble_accuracy:+.4f})\")\n",
    "print(f\"Log Loss: {best_accuracy['log_loss']:.4f} (Δ: {best_accuracy['log_loss'] - ensemble_log_loss:+.4f})\")\n",
    "print(f\"Changes: {best_accuracy['changes']} ({best_accuracy['change_percent']:.1f}%)\")\n",
    "print(f\"Interventions: {best_accuracy['interventions']} ({best_accuracy['intervention_rate']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nBest Log Loss Configuration:\")\n",
    "print(f\"Confidence Threshold: {best_log_loss['confidence_threshold']:.2f}\")\n",
    "print(f\"Boost Probability: {best_log_loss['boost_probability']:.2f}\")\n",
    "print(f\"Accuracy: {best_log_loss['accuracy']:.4f} (Δ: {best_log_loss['accuracy'] - ensemble_accuracy:+.4f})\")\n",
    "print(f\"Log Loss: {best_log_loss['log_loss']:.4f} (Δ: {best_log_loss['log_loss'] - ensemble_log_loss:+.4f})\")\n",
    "print(f\"Changes: {best_log_loss['changes']} ({best_log_loss['change_percent']:.1f}%)\")\n",
    "print(f\"Interventions: {best_log_loss['interventions']} ({best_log_loss['intervention_rate']:.1f}%)\")\n",
    "\n",
    "# Top 10 configurations by accuracy\n",
    "print(f\"\\nTop 10 Configurations by Accuracy:\")\n",
    "top_accuracy = results_df.nlargest(10, 'accuracy')[['confidence_threshold', 'boost_probability', 'accuracy', 'log_loss', 'change_percent']]\n",
    "print(top_accuracy.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Top 10 configurations by log loss (lowest)\n",
    "print(f\"\\nTop 10 Configurations by Log Loss (lowest):\")\n",
    "top_log_loss = results_df.nsmallest(10, 'log_loss')[['confidence_threshold', 'boost_probability', 'accuracy', 'log_loss', 'change_percent']]\n",
    "print(top_log_loss.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd61a78d272ae14",
   "metadata": {},
   "source": [
    "## 12. Generate Final Predictions for Test Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bad4c2ca09dba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating features for test products...\n",
      "\n",
      "Extracting graph features for test products...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "Graph features shape for test products: (45502, 5)\n",
      "Node2Vec features shape for test products: (45502, 128)\n",
      "Price features shape for test products: (45502, 7)\n",
      "\n",
      "Combining all features...\n",
      "Combined features shape for test products: (45502, 2060153)\n",
      "\n",
      "Making predictions using all models...\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 72ms/step\n",
      "Applying specialist model logic to final predictions...\n",
      "Predictions shape: (45502, 16)\n",
      "Predictions saved to predictions_with_specialists.csv\n",
      "\n",
      "Sample of predictions:\n",
      "  product  class0  class1  class2  class3  class4  class5  class6  class7  \\\n",
      "0   49957  0.0004  0.0135  0.0107  0.0002  0.0040  0.0461  0.0144  0.0029   \n",
      "1  135386  0.0001  0.0003  0.0001  0.0000  0.0008  0.0456  0.0004  0.0013   \n",
      "2  226880  0.0000  0.0007  0.9903  0.0000  0.0043  0.0004  0.0002  0.0016   \n",
      "3  165114  0.0000  0.0025  0.0184  0.0001  0.0086  0.0002  0.0002  0.0023   \n",
      "4  256154  0.0000  0.0007  0.9912  0.0001  0.0007  0.0002  0.0009  0.0011   \n",
      "\n",
      "   class8  class9  class10  class11  class12  class13  class14  class15  \n",
      "0  0.0008  0.0006   0.9000   0.0015   0.0030   0.0007   0.0001   0.0012  \n",
      "1  0.0001  0.9504   0.0004   0.0001   0.0002   0.0001   0.0000   0.0000  \n",
      "2  0.0001  0.0001   0.0014   0.0002   0.0004   0.0000   0.0001   0.0001  \n",
      "3  0.0004  0.0008   0.9658   0.0002   0.0000   0.0001   0.0000   0.0004  \n",
      "4  0.0001  0.0001   0.0005   0.0007   0.0032   0.0001   0.0002   0.0001  \n"
     ]
    }
   ],
   "source": [
    "# Create features for test products\n",
    "print(\"\\nCreating features for test products...\")\n",
    "\n",
    "# 1. Create graph features for test products\n",
    "print(\"\\nExtracting graph features for test products...\")\n",
    "test_graph_features_final = extract_graph_features(G, test_products)\n",
    "test_graph_features_final = test_graph_features_final.fillna(0)\n",
    "test_graph_features_final_scaled = graph_scaler.transform(test_graph_features_final)\n",
    "test_graph_sparse_final = csr_matrix(test_graph_features_final_scaled)\n",
    "print(f\"Graph features shape for test products: {test_graph_sparse_final.shape}\")\n",
    "\n",
    "# 2. Create Node2Vec embeddings for test products\n",
    "test_node2vec_features_final = np.zeros((len(test_products), 128))\n",
    "for i, node_id in enumerate(test_products):\n",
    "    try:\n",
    "        test_node2vec_features_final[i] = n2v_model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in embeddings, use zeros\n",
    "        pass\n",
    "\n",
    "# Scale the Node2Vec embeddings\n",
    "test_node2vec_scaled_final = n2v_scaler.transform(test_node2vec_features_final)\n",
    "test_node2vec_sparse_final = csr_matrix(test_node2vec_scaled_final)\n",
    "print(f\"Node2Vec features shape for test products: {test_node2vec_sparse_final.shape}\")\n",
    "\n",
    "# 3. Create price features for test products\n",
    "test_price_features_final = create_price_features(test_products, price_df)\n",
    "test_price_scaled_final = test_price_features_final.copy()\n",
    "test_price_scaled_final[price_columns_to_scale] = price_scaler.transform(test_price_features_final[price_columns_to_scale])\n",
    "test_price_features_array_final = test_price_scaled_final.values\n",
    "test_price_sparse_final = csr_matrix(test_price_features_array_final)\n",
    "print(f\"Price features shape for test products: {test_price_sparse_final.shape}\")\n",
    "\n",
    "# COMBINE ALL FEATURES\n",
    "print(\"\\nCombining all features...\")\n",
    "X_combined_test_final = hstack([\n",
    "    X_tfidf_test_comp, \n",
    "    test_graph_sparse_final,\n",
    "    test_node2vec_sparse_final, \n",
    "    test_price_sparse_final\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Combined features shape for test products: {X_combined_test_final.shape}\")\n",
    "\n",
    "# Apply feature selection for different models\n",
    "X_combined_test_final_selected = feature_selector.transform((X_combined_test_final))\n",
    "X_combined_test_final_selected_svm = feature_selector_svm.transform((X_combined_test_final))\n",
    "\n",
    "# Make predictions using all models\n",
    "print(\"\\nMaking predictions using all models...\")\n",
    "test_pred_proba_svm = svm_model.predict_proba(X_combined_test_final_selected_svm)\n",
    "test_pred_proba_xgb = xgb_model.predict_proba(X_combined_test_final_selected)\n",
    "test_pred_proba_nn = nn_model.predict(X_combined_test_final_selected)\n",
    "\n",
    "# Combine predictions with weighted average\n",
    "test_pred_proba_ensemble = (\n",
    "    weights[0] * test_pred_proba_svm + \n",
    "    weights[1] * test_pred_proba_xgb + \n",
    "    weights[2] * test_pred_proba_nn\n",
    ")\n",
    "\n",
    "# Apply specialist model logic to the ensemble predictions\n",
    "print(\"Applying specialist model logic to final predictions...\")\n",
    "# Create a dictionary of specialist models for the final test set\n",
    "final_specialist_models = {}\n",
    "for specialist_class in specialist_classes:\n",
    "    model_filename = f'specialist_model_class_{specialist_class}.pkl'\n",
    "    final_specialist_models[specialist_class] = joblib.load(model_filename)\n",
    "\n",
    "# Define a function to apply specialist models to the test set\n",
    "def apply_specialist_models_to_test(ensemble_probs, specialist_models, specialist_classes, features, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Apply specialist model logic to ensemble predictions for the test set\n",
    "    \n",
    "    Args:\n",
    "        ensemble_probs: Array of shape (n_samples, n_classes) with ensemble probabilities\n",
    "        specialist_models: Dictionary mapping class to specialist model\n",
    "        specialist_classes: List of classes that have specialist models\n",
    "        features: Features to use for specialist model predictions\n",
    "        threshold: Probability threshold below which to consult specialist models\n",
    "        \n",
    "    Returns:\n",
    "        Modified probability array\n",
    "    \"\"\"\n",
    "    # Make a copy of the input probabilities to avoid modifying the original\n",
    "    modified_probs = ensemble_probs.copy()\n",
    "    \n",
    "    # For each sample\n",
    "    for i in range(ensemble_probs.shape[0]):\n",
    "        # Get the predicted class and its probability\n",
    "        pred_class = np.argmax(ensemble_probs[i])\n",
    "        pred_prob = ensemble_probs[i, pred_class]\n",
    "        \n",
    "        # Check if the predicted class is one of the specialist classes and probability is below threshold\n",
    "        if pred_class in specialist_classes and pred_prob < threshold:\n",
    "            # Get the specialist model for this class\n",
    "            specialist_model = specialist_models[pred_class]\n",
    "            \n",
    "            # Extract the features for this sample\n",
    "            sample_features = features[i]\n",
    "            \n",
    "            # Reshape to 2D array for prediction\n",
    "            sample_features_2d = sample_features.reshape(1, -1)\n",
    "            \n",
    "            # Get the specialist's prediction (probability of being the specialist class)\n",
    "            specialist_prob = specialist_model.predict_proba(sample_features_2d)[0, 1]\n",
    "            \n",
    "            # If the specialist agrees (probability > 0.5)\n",
    "            if specialist_prob > 0.5:\n",
    "                # Bump the probability up to 0.9\n",
    "                old_prob = modified_probs[i, pred_class]\n",
    "                modified_probs[i, pred_class] = threshold\n",
    "                \n",
    "                # Calculate how much probability was added\n",
    "                prob_added = threshold - old_prob\n",
    "                \n",
    "                # Distribute the added probability proportionally from other classes\n",
    "                # to maintain sum = 1\n",
    "                for j in range(ensemble_probs.shape[1]):\n",
    "                    if j != pred_class:\n",
    "                        # Reduce other probabilities proportionally\n",
    "                        modified_probs[i, j] = modified_probs[i, j] * (1 - threshold) / (1 - old_prob)\n",
    "            else:\n",
    "                # Specialist disagrees, swap the two highest scores\n",
    "                # Find the indices of the two highest probabilities\n",
    "                top_indices = np.argsort(ensemble_probs[i])[-2:]\n",
    "                \n",
    "                # Swap the values\n",
    "                temp = modified_probs[i, top_indices[0]]\n",
    "                modified_probs[i, top_indices[0]] = modified_probs[i, top_indices[1]]\n",
    "                modified_probs[i, top_indices[1]] = temp\n",
    "    \n",
    "    return modified_probs\n",
    "\n",
    "# Apply the specialist model logic to the final test predictions\n",
    "modified_test_pred_proba_ensemble = apply_specialist_models_to_test(\n",
    "    test_pred_proba_ensemble, \n",
    "    final_specialist_models, \n",
    "    specialist_classes, \n",
    "    X_combined_test_final_selected,\n",
    "    threshold=0.9\n",
    ")\n",
    "\n",
    "print(f\"Predictions shape: {modified_test_pred_proba_ensemble.shape}\")\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['product'] = test_products\n",
    "\n",
    "# Add probability for each class\n",
    "for i in range(modified_test_pred_proba_ensemble.shape[1]):\n",
    "    predictions_df[f'class{i}'] = modified_test_pred_proba_ensemble[:, i].round(4)\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('predictions_with_specialists.csv', index=False)\n",
    "print(f\"Predictions saved to predictions_with_specialists.csv\")\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(\"\\nSample of predictions:\")\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99374d535decd577",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b17a21b2c5a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of Specialist Models Implementation:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Specialist models were trained for classes: {specialist_classes}\")\n",
    "print(f\"Threshold for consulting specialist models: 0.9\")\n",
    "print(f\"Original Ensemble Accuracy: {ensemble_accuracy:.4f}, Log Loss: {ensemble_log_loss:.4f}\")\n",
    "print(f\"Modified Ensemble Accuracy: {modified_ensemble_accuracy:.4f}, Log Loss: {modified_ensemble_log_loss:.4f}\")\n",
    "print(f\"Number of predictions changed by specialist models: {changes} ({changes/len(y_test)*100:.2f}%)\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Specialist Model Logic:\")\n",
    "print(\"1. If the ensemble model predicts class 1, 10, or 12 with probability < 0.9:\")\n",
    "print(\"   a. Consult the specialist model for that class\")\n",
    "print(\"   b. If specialist agrees (prob > 0.5), bump probability to 0.9\")\n",
    "print(\"   c. If specialist disagrees, swap the two highest probability scores\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Final predictions saved to: predictions_with_specialists.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
