{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph",
   "id": "27482f3e4d38371a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "52f94bc4e259071b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import node2vec\n",
    "from karateclub import DeepWalk\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load files",
   "id": "713376102aca7571"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the edge list data\n",
    "edgelist_file = 'data_files/edgelist.txt'\n",
    "edges_df = pd.read_csv(edgelist_file, header=None, names=['source', 'target'])\n",
    "\n",
    "# Load the class labels\n",
    "labels_file = 'y_train.txt'\n",
    "labels_df = pd.read_csv(labels_file, header=None, names=['product_id', 'label'])\n",
    "\n",
    "# Load the train and test splits\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Edge list shape: {edges_df.shape}\")\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# Check the first few rows of each dataset\n",
    "print(\"\\nEdge list sample:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nLabels sample:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "print(\"\\nTrain set sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(test_df.head())\n"
   ],
   "id": "48cef30b55d07477"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Graph",
   "id": "8e00455bd070641e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a graph from the edge list\n",
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f\"Is the graph connected? {is_connected}\")\n",
    "\n",
    "if not is_connected:\n",
    "    # Get the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    print(f\"\\nLargest Connected Component:\")\n",
    "    print(f\"  Nodes: {largest_cc_subgraph.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {largest_cc_subgraph.number_of_edges()}\")\n",
    "    print(f\"  Percentage of total nodes: {largest_cc_subgraph.number_of_nodes() / G.number_of_nodes() * 100:.2f}%\")\n"
   ],
   "id": "24b23258d4abe4bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare train test",
   "id": "419b02c5eb101153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the list of product IDs from train and test sets\n",
    "train_product_ids = train_df['product_id'].tolist()\n",
    "test_product_ids = test_df['product_id'].tolist()\n",
    "\n",
    "# Extract features for training and testing sets\n",
    "print(\"Extracting features for training set...\")\n",
    "train_features = extract_graph_features(G, train_product_ids)\n",
    "\n",
    "print(\"\\nExtracting features for testing set...\")\n",
    "test_features = extract_graph_features(G, test_product_ids)\n",
    "\n",
    "# Get the labels for training and testing sets\n",
    "train_labels = train_df['label'].values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Display the feature dataframes\n",
    "print(\"\\nTraining features shape:\", train_features.shape)\n",
    "print(\"Testing features shape:\", test_features.shape)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training features:\", train_features.isnull().sum().sum())\n",
    "print(\"Missing values in testing features:\", test_features.isnull().sum().sum())\n",
    "\n",
    "# Fill missing values with 0 if any\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n"
   ],
   "id": "588539c7e93845bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Node2Vec Embeddings + XGBoost",
   "id": "ed2377ceebae9f1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate Node2Vec embeddings\n",
    "print(\"Generating Node2Vec embeddings...\")\n",
    "\n",
    "try:\n",
    "    # Convert NetworkX graph to node2vec format\n",
    "    # First, ensure all nodes are strings for compatibility\n",
    "    G_node2vec = nx.Graph()\n",
    "    for edge in G.edges():\n",
    "        G_node2vec.add_edge(str(edge[0]), str(edge[1]))\n",
    "\n",
    "    # Initialize node2vec model\n",
    "    node2vec_model = node2vec.Node2Vec(\n",
    "        G_node2vec,\n",
    "        dimensions=64,  # Embedding dimension\n",
    "        walk_length=5,  # Length of each random walk\n",
    "        num_walks=5,    # Number of random walks per node\n",
    "        workers=1        # Number of parallel workers\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training Node2Vec model...\")\n",
    "    n2v_model = node2vec_model.fit(\n",
    "        window=10,       # Context size for optimization\n",
    "        min_count=1,     # Minimum count of node occurrences\n",
    "        batch_words=4    # Number of words per batch\n",
    "    )\n",
    "\n",
    "    # Generate embeddings for train and test nodes\n",
    "    train_node2vec_features = np.zeros((len(train_product_ids), 64))\n",
    "    test_node2vec_features = np.zeros((len(test_product_ids), 64))\n",
    "\n",
    "    # Extract embeddings for training nodes\n",
    "    for i, node_id in enumerate(train_product_ids):\n",
    "        try:\n",
    "            train_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    # Extract embeddings for testing nodes\n",
    "    for i, node_id in enumerate(test_product_ids):\n",
    "        try:\n",
    "            test_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    print(f\"Node2Vec embeddings shape - Train: {train_node2vec_features.shape}, Test: {test_node2vec_features.shape}\")\n",
    "\n",
    "    # Scale the embeddings\n",
    "    n2v_scaler = StandardScaler()\n",
    "    train_node2vec_scaled = n2v_scaler.fit_transform(train_node2vec_features)\n",
    "    test_node2vec_scaled = n2v_scaler.transform(test_node2vec_features)\n",
    "\n",
    "    # Train a classifier on Node2Vec embeddings\n",
    "    print(\"Training XGBoost on Node2Vec embeddings...\")\n",
    "    n2v_xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    n2v_xgb_model.fit(train_node2vec_scaled, train_labels)\n",
    "\n",
    "    # Make predictions\n",
    "    n2v_xgb_test_pred = n2v_xgb_model.predict(test_node2vec_scaled)\n",
    "    n2v_xgb_test_pred_proba = n2v_xgb_model.predict_proba(test_node2vec_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    n2v_xgb_accuracy = accuracy_score(test_labels, n2v_xgb_test_pred)\n",
    "    n2v_xgb_loss = log_loss(test_labels, n2v_xgb_test_pred_proba)\n",
    "\n",
    "    print(f\"Node2Vec XGBoost Test Accuracy: {n2v_xgb_accuracy:.4f}\")\n",
    "    print(f\"Node2Vec XGBoost Multi-class Log Loss: {n2v_xgb_loss:.4f}\")\n",
    "    print(\"\\nNode2Vec XGBoost Classification Report:\")\n",
    "    print(classification_report(test_labels, n2v_xgb_test_pred))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Node2Vec embeddings: {e}\")\n",
    "    print(\"Skipping Node2Vec embeddings...\")\n"
   ],
   "id": "26cf903c1a53299a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictions for the test dataset (the one that we actually want to send)",
   "id": "a917cf83afd18d23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load product IDs from test.txt\n",
    "print(\"Loading product IDs from test.txt...\")\n",
    "test_txt_products = []\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        test_txt_products.append(int(t[0]))\n",
    "\n",
    "print(f\"Loaded {len(test_txt_products)} product IDs from test.txt\")\n",
    "\n",
    "# Generate Node2Vec embeddings for test.txt products\n",
    "print(\"Generating Node2Vec embeddings for test.txt products...\")\n",
    "test_txt_node2vec_features = np.zeros((len(test_txt_products), 64))\n",
    "\n",
    "# Extract embeddings for test.txt products\n",
    "for i, node_id in enumerate(test_txt_products):\n",
    "    try:\n",
    "        test_txt_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in embeddings, use zeros\n",
    "        pass\n",
    "\n",
    "print(f\"Node2Vec embeddings shape for test.txt products: {test_txt_node2vec_features.shape}\")\n",
    "\n",
    "# Scale the embeddings using the same scaler used for training\n",
    "test_txt_node2vec_scaled = n2v_scaler.transform(test_txt_node2vec_features)\n",
    "\n",
    "# Make predictions using the trained Node2Vec XGBoost model\n",
    "print(\"Making predictions for test.txt products...\")\n",
    "test_txt_pred_proba = n2v_xgb_model.predict_proba(test_txt_node2vec_scaled)\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "print(\"Creating CSV with predictions...\")\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['product'] = test_txt_products\n",
    "\n",
    "# Add probability columns for each class\n",
    "for i in range(len(n2v_xgb_model.classes_)):\n",
    "    predictions_df[f'class{i}'] = test_txt_pred_proba[:, i].round(4)\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('node2vec_predictions.csv', index=False)\n",
    "print(f\"Predictions saved to node2vec_predictions.csv\")\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(\"\\nSample of predictions:\")\n",
    "print(predictions_df.head())\n"
   ],
   "id": "b8178f577eb68bc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Description - Text",
   "id": "c4565ee981c633d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Find rows with missing 'description' in train_df\n",
    "missing_train = train_df[train_df['text_clean'].isnull()]\n",
    "\n",
    "# Find rows with missing 'description' in test_df\n",
    "missing_test = test_df[test_df['text_clean'].isnull()]\n",
    "\n",
    "# Print the rows with missing values\n",
    "print(\"Rows with missing 'description' in train_df:\")\n",
    "print(missing_train)\n",
    "\n",
    "print(\"\\nRows with missing 'description' in test_df:\")\n",
    "print(missing_test)\n",
    "\n",
    "\n",
    "# Remove rows with missing 'description' in train_df\n",
    "train_df = train_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Remove rows with missing 'description' in test_df\n",
    "test_df = test_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Verify if any rows with missing values remain\n",
    "print(\"Missing values in cleaned train_df:\", train_df.isnull().sum())\n",
    "print(\"Missing values in cleaned test_df:\", test_df.isnull().sum())\n",
    "\n",
    "# Reset the index after removing rows with missing values\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the first few rows to confirm that the index is reset\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "\n",
    "train_df.columns = ['product_id', 'description', 'label']\n",
    "test_df.columns = ['product_id', 'description', 'label']\n",
    "\n",
    "display(HTML(train_df.tail(5).to_html(escape=False)))"
   ],
   "id": "52a448e24301cf24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train = train_df['description']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['description']\n",
    "y_test = test_df['label']"
   ],
   "id": "48dd29c74a45b4f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.95)\n",
    "\n",
    "\n",
    "# Fit and transform the text data to get the TF-IDF matrix\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train) # labeled train set\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test) # labeled test set\n",
    "#X_tfidf_test_comp =  tfidf_vectorizer.transform(test_text_cleaned) # unlabeled test set\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "X_embed_train = model.encode(X_train, convert_to_tensor=False)\n",
    "X_embed_test = model.encode(X_test, convert_to_tensor=False)\n",
    "\n",
    "# Concatenate TF-IDF with semantic embeddings\n",
    "from scipy.sparse import hstack\n",
    "X_comb_train = hstack([X_tfidf_train, X_embed_train])\n",
    "X_comb_test = hstack([X_tfidf_test, X_embed_test])\n",
    "\n",
    "\n",
    "#---------TRAIN BEST SVC-------------------------------------\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "base_svm2 = LinearSVC(max_iter=5000)\n",
    "svm_model2 = CalibratedClassifierCV(base_svm2, cv=5)  # Platt scaling internally\n",
    "\n",
    "svm_model2.fit(X_comb_train, y_train)\n",
    "\n",
    "y_pred_svm2 = svm_model2.predict(X_comb_test)\n",
    "print(\"\\nSVM Final Test Set Performance:\")\n",
    "\n",
    "\n",
    "y_proba_svm2 = svm_model2.predict_proba(X_comb_test)\n",
    "\n",
    "# Compute custom log loss\n",
    "custom_loss_svm2 = multiclass_log_loss(y_test, y_proba_svm2)\n",
    "print(f\"Probabilistic Log Loss (SVM): {custom_loss_svm2:.4f}\")"
   ],
   "id": "df23dfc8cab28fe7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
