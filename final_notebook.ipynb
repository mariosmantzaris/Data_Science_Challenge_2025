{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08f0464adefa0fa",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483a5da150f35a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:06:13.013375Z",
     "start_time": "2025-05-31T16:06:07.564513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 19:06:10.656034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748707570.718483   39746 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748707570.736197   39746 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748707570.868128   39746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748707570.868175   39746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748707570.868178   39746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748707570.868181   39746 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 19:06:10.884021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Graph embeddings\n",
    "import node2vec\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5029ebaaba5b",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a2f3e191173afb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:06:13.036866Z",
     "start_time": "2025-05-31T16:06:13.028537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create price features\n",
    "def create_price_features(product_ids, price_df):\n",
    "    \"\"\"\n",
    "    Create price-based features for a list of product IDs\n",
    "\n",
    "    Args:\n",
    "        product_ids: List of product IDs\n",
    "        price_df: DataFrame with product_id and price columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with price features\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with product IDs as index\n",
    "    price_features = pd.DataFrame(index=product_ids)\n",
    "\n",
    "    # Map prices to products\n",
    "    price_dict = dict(zip(price_df['product_id'], price_df['price']))\n",
    "    price_features['price'] = price_features.index.map(lambda x: price_dict.get(x, np.nan))\n",
    "\n",
    "    # Fill missing prices with median\n",
    "    median_price = price_df['price'].median()\n",
    "    price_features['price'].fillna(median_price, inplace=True)\n",
    "\n",
    "    # Create price buckets (as binary features)\n",
    "    price_features['price_0_10'] = (price_features['price'] <= 10).astype(int)\n",
    "    price_features['price_10_100'] = ((price_features['price'] > 10) & (price_features['price'] <= 100)).astype(int)\n",
    "    price_features['price_100_plus'] = (price_features['price'] > 100).astype(int)\n",
    "\n",
    "    # Log transformation of price\n",
    "    price_features['price_log'] = np.log1p(price_features['price'])\n",
    "\n",
    "    # Price rank (percentile)\n",
    "    price_features['price_rank'] = price_features['price'].rank(pct=True)\n",
    "\n",
    "    # Z-score of price (how many standard deviations from the mean)\n",
    "    mean_price = price_df['price'].mean()\n",
    "    std_price = price_df['price'].std()\n",
    "    price_features['price_zscore'] = (price_features['price'] - mean_price) / std_price\n",
    "\n",
    "    return price_features\n",
    "\n",
    "# Function to extract graph features for a set of nodes\n",
    "def extract_graph_features(G, node_list):\n",
    "    print(\"Calculating degree centrality...\")\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    print(\"Calculating clustering coefficient...\")\n",
    "    clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "    print(\"Calculating PageRank...\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "\n",
    "    print(\"Calculating triangle count...\")\n",
    "    triangles = nx.triangles(G)\n",
    "\n",
    "    # Create a dataframe with the features\n",
    "    features_df = pd.DataFrame(index=node_list)\n",
    "\n",
    "    features_df['degree_centrality'] = features_df.index.map(lambda x: degree_centrality.get(str(x), 0))\n",
    "    features_df['clustering_coefficient'] = features_df.index.map(lambda x: clustering_coefficient.get(str(x), 0))\n",
    "    features_df['pagerank'] = features_df.index.map(lambda x: pagerank.get(str(x), 0))\n",
    "    features_df['triangle_count'] = features_df.index.map(lambda x: triangles.get(str(x), 0))\n",
    "\n",
    "    # Degree (number of connections)\n",
    "    print(\"Calculating degree...\")\n",
    "    degree_dict = dict(G.degree())\n",
    "    features_df['degree'] = features_df.index.map(lambda x: degree_dict.get(str(x), 0))\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Text preprocessing function with lemmatization\n",
    "def clean_text_with_lemma(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Use spaCy to tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if token.lemma_.lower() not in STOP_WORDS\n",
    "        and not token.is_punct\n",
    "        and not token.is_space\n",
    "        and not token.like_num\n",
    "    ]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to calculate multiclass log loss\n",
    "def multiclass_log_loss(y_true, y_pred_proba, eps=1e-15):\n",
    "    \"\"\"\n",
    "    y_true: array-like of shape (N,) - true class labels\n",
    "    y_pred_proba: array-like of shape (N, C) - predicted class probabilities\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    # One-hot encode the true labels (yij)\n",
    "    y_true_one_hot = label_binarize(y_true, classes=np.arange(y_pred_proba.shape[1]))\n",
    "\n",
    "    # Clip predicted probabilities to avoid log(0)\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "\n",
    "    # Compute the log loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred_proba)) / N\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44029a942a9d02",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488c6142ca72317e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:06:17.407769Z",
     "start_time": "2025-05-31T16:06:13.288912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge list shape: (1811087, 2)\n",
      "Labels shape: (182006, 2)\n",
      "Train set shape: (145604, 3)\n",
      "Test set shape: (36402, 3)\n",
      "Price data shape: (198817, 2)\n",
      "\n",
      "Edge list sample:\n",
      "   source  target\n",
      "0  251528  237411\n",
      "1  100805   74791\n",
      "2   38634   97747\n",
      "3  247470   77089\n",
      "4  267060  250490\n",
      "\n",
      "Labels sample:\n",
      "   product_id  label\n",
      "0       66795      9\n",
      "1      242781      3\n",
      "2       91280      2\n",
      "3       56356      5\n",
      "4      218494      0\n",
      "\n",
      "Train set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0      114704  hornady unprimed winchester cartridge case hor...      2\n",
      "1      250731  tachikara tk leopard knee pad tachikara tk leo...     11\n",
      "2      152967  g asd replacement cutter aluminum amp carbon u...      2\n",
      "3        4541  mtech usa mt tactical folding knife inch close...      2\n",
      "4      142062  nhl pittsburgh penguins game day black pro sha...      7\n",
      "\n",
      "Test set sample:\n",
      "   product_id                                         text_clean  label\n",
      "0       56218                             katz hoodie volleyball      0\n",
      "1       42346  vz grip operator ii standard size gun grip usa...      2\n",
      "2      215842  tough flat leather hobble tough flat leather h...     15\n",
      "3       36062  buzzrack skipper bicycle suv hatchback wheel c...      1\n",
      "4      188250  cuisinart cgg allfood btu portable outdoor tab...     10\n",
      "\n",
      "Price data sample:\n",
      "   product_id   price\n",
      "0           0   50.50\n",
      "1           2   17.97\n",
      "2           3    4.99\n",
      "3           4   36.60\n",
      "4           5  199.95\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy English model for text preprocessing\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the edge list data\n",
    "edgelist_file = 'data_files/edgelist.txt'\n",
    "edges_df = pd.read_csv(edgelist_file, header=None, names=['source', 'target'])\n",
    "\n",
    "# Load the class labels\n",
    "labels_file = 'y_train.txt'\n",
    "labels_df = pd.read_csv(labels_file, header=None, names=['product_id', 'label'])\n",
    "\n",
    "# Load the train and test splits\n",
    "train_df = pd.read_csv('split_dataset/train.csv')\n",
    "test_df = pd.read_csv('split_dataset/test.csv')\n",
    "\n",
    "# Load price data\n",
    "price_df = pd.read_csv('data_files/price.txt', header=None, names=['product_id', 'price'])\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(f\"Edge list shape: {edges_df.shape}\")\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Price data shape: {price_df.shape}\")\n",
    "\n",
    "# Check the first few rows of each dataset\n",
    "print(\"\\nEdge list sample:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nLabels sample:\")\n",
    "print(labels_df.head())\n",
    "\n",
    "print(\"\\nTrain set sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest set sample:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nPrice data sample:\")\n",
    "print(price_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159119bac272ec69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:12:51.209854Z",
     "start_time": "2025-05-31T16:06:17.425680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing 'description' in train_df:\n",
      "       product_id text_clean  label\n",
      "89285      265165        NaN      7\n",
      "\n",
      "Rows with missing 'description' in test_df:\n",
      "       product_id text_clean  label\n",
      "34767      174103        NaN      5\n",
      "Missing values in cleaned train_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Missing values in cleaned test_df: product_id    0\n",
      "text_clean    0\n",
      "label         0\n",
      "dtype: int64\n",
      "Loading product IDs from test.txt...\n",
      "Loaded 45502 product IDs from test.txt\n",
      "First 10 product IDs: ['49957', '135386', '226880', '165114', '256154', '254193', '20830', '46170', '19248', '158023']\n",
      "Getting descriptions\n",
      "Apply cleaning\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in text data\n",
    "# Find rows with missing 'description' in train_df\n",
    "missing_train = train_df[train_df['text_clean'].isnull()]\n",
    "\n",
    "# Find rows with missing 'description' in test_df\n",
    "missing_test = test_df[test_df['text_clean'].isnull()]\n",
    "\n",
    "# Print the rows with missing values\n",
    "print(\"Rows with missing 'description' in train_df:\")\n",
    "print(missing_train)\n",
    "\n",
    "print(\"\\nRows with missing 'description' in test_df:\")\n",
    "print(missing_test)\n",
    "\n",
    "# Remove rows with missing 'description' in train_df\n",
    "train_df = train_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Remove rows with missing 'description' in test_df\n",
    "test_df = test_df.dropna(subset=['text_clean'])\n",
    "\n",
    "# Verify if any rows with missing values remain\n",
    "print(\"Missing values in cleaned train_df:\", train_df.isnull().sum())\n",
    "print(\"Missing values in cleaned test_df:\", test_df.isnull().sum())\n",
    "\n",
    "# Reset the index after removing rows with missing values\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Load product IDs from test.txt for final predictions\n",
    "print(\"Loading product IDs from test.txt...\")\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_products = [line.strip().rstrip(',') for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(test_products)} product IDs from test.txt\")\n",
    "print(\"First 10 product IDs:\", test_products[:10])\n",
    "\n",
    "# Load descriptions for all products\n",
    "print(\"Getting descriptions\")\n",
    "descriptions = dict()\n",
    "with open(\"data_files/description_part_1.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "with open(\"data_files/description_part_2.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if '|=|' in line:\n",
    "            t = line.split('|=|')\n",
    "            descriptions[int(t[0])] = t[1][:-1]\n",
    "\n",
    "# Get descriptions for test products\n",
    "test_text = []\n",
    "for i in test_products:\n",
    "    try:\n",
    "        test_text.append(descriptions[int(i)])\n",
    "    except (KeyError, ValueError):\n",
    "        test_text.append(\"\")  # Empty string for missing descriptions\n",
    "\n",
    "# Apply the cleaning function to test data\n",
    "print(\"Apply cleaning\")\n",
    "test_text_cleaned = [clean_text_with_lemma(text) for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914741f5b3413f0b",
   "metadata": {},
   "source": [
    "## 4. Creating Graph and Extracting Graph Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cf1ae5a030a8cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:14:18.383067Z",
     "start_time": "2025-05-31T16:12:51.280012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph\n",
      "Number of nodes: 276453\n",
      "Number of edges: 1811087\n",
      "Network Density: 0.000047\n",
      "Is the graph connected? False\n",
      "\n",
      "Largest Connected Component:\n",
      "  Nodes: 273012\n",
      "  Edges: 1808230\n",
      "  Percentage of total nodes: 98.76%\n",
      "\n",
      "Extracting graph features for training set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "\n",
      "Extracting graph features for testing set...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "Loading Node2Vec embeddings...\n",
      "Loading existing Node2Vec model from disk...\n",
      "Node2Vec embeddings shape - Train: (145603, 128), Test: (36401, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create a graph from the edge list\n",
    "print(\"Creating graph\")\n",
    "G = nx.from_pandas_edgelist(edges_df, 'source', 'target')\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "is_connected = nx.is_connected(G)\n",
    "print(f\"Is the graph connected? {is_connected}\")\n",
    "\n",
    "if not is_connected:\n",
    "    # Get the largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    print(f\"\\nLargest Connected Component:\")\n",
    "    print(f\"  Nodes: {largest_cc_subgraph.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {largest_cc_subgraph.number_of_edges()}\")\n",
    "    print(f\"  Percentage of total nodes: {largest_cc_subgraph.number_of_nodes() / G.number_of_nodes() * 100:.2f}%\")\n",
    "\n",
    "# Get the list of product IDs from train and test sets\n",
    "train_product_ids = train_df['product_id'].tolist()\n",
    "test_product_ids = test_df['product_id'].tolist()\n",
    "\n",
    "# Extract graph features for training and testing sets\n",
    "print(\"\\nExtracting graph features for training set...\")\n",
    "train_graph_features = extract_graph_features(G, train_product_ids)\n",
    "\n",
    "print(\"\\nExtracting graph features for testing set...\")\n",
    "test_graph_features = extract_graph_features(G, test_product_ids)\n",
    "\n",
    "# Fill missing values with 0 if any\n",
    "train_graph_features = train_graph_features.fillna(0)\n",
    "test_graph_features = test_graph_features.fillna(0)\n",
    "\n",
    "# Scale the features\n",
    "graph_scaler = StandardScaler()\n",
    "train_graph_features_scaled = graph_scaler.fit_transform(train_graph_features)\n",
    "test_graph_features_scaled = graph_scaler.transform(test_graph_features)\n",
    "\n",
    "# Generate Node2Vec embeddings\n",
    "model_path = \"node2vec.model\"\n",
    "print(\"Loading Node2Vec embeddings...\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading existing Node2Vec model from disk...\")\n",
    "        n2v_model = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        # Convert NetworkX graph to node2vec format\n",
    "        # First, ensure all nodes are strings for compatibility\n",
    "        G_node2vec = nx.Graph()\n",
    "        for edge in G.edges():\n",
    "            G_node2vec.add_edge(str(edge[0]), str(edge[1]))\n",
    "\n",
    "        # Initialize node2vec model\n",
    "        node2vec_model = node2vec.Node2Vec(\n",
    "            G_node2vec,\n",
    "            dimensions=128,  # Embedding dimension\n",
    "            walk_length=10,  # Length of each random walk\n",
    "            num_walks=10,    # Number of random walks per node\n",
    "            workers=1       # Number of parallel workers\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Training Node2Vec model...\")\n",
    "        n2v_model = node2vec_model.fit(\n",
    "            window=10,       # Context size for optimization\n",
    "            min_count=1,     # Minimum count of node occurrences\n",
    "            batch_words=4    # Number of words per batch\n",
    "        )\n",
    "\n",
    "        n2v_model.save(model_path)\n",
    "\n",
    "    # Generate embeddings for train and test nodes\n",
    "    train_node2vec_features = np.zeros((len(train_product_ids), 128))\n",
    "    test_node2vec_features = np.zeros((len(test_product_ids), 128))\n",
    "\n",
    "    # Extract embeddings for training nodes\n",
    "    for i, node_id in enumerate(train_product_ids):\n",
    "        try:\n",
    "            train_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    # Extract embeddings for testing nodes\n",
    "    for i, node_id in enumerate(test_product_ids):\n",
    "        try:\n",
    "            test_node2vec_features[i] = n2v_model.wv[str(node_id)]\n",
    "        except KeyError:\n",
    "            # If node not in embeddings, use zeros\n",
    "            pass\n",
    "\n",
    "    print(f\"Node2Vec embeddings shape - Train: {train_node2vec_features.shape}, Test: {test_node2vec_features.shape}\")\n",
    "\n",
    "    # Scale the embeddings\n",
    "    n2v_scaler = StandardScaler()\n",
    "    train_node2vec_scaled = n2v_scaler.fit_transform(train_node2vec_features)\n",
    "    test_node2vec_scaled = n2v_scaler.transform(test_node2vec_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating Node2Vec embeddings: {e}\")\n",
    "    print(\"Skipping Node2Vec embeddings...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a33f607c724795",
   "metadata": {},
   "source": [
    "## 5. Extracting TF-IDF Features from Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940d8719b40a006d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:14:37.881989Z",
     "start_time": "2025-05-31T16:14:21.285885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TF-IDF\n",
      "TF-IDF features shape - Train: (145603, 2060013), Test: (36401, 2060013)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing TF-IDF\")\n",
    "# Initialize the TfidfVectorizer with parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95, sublinear_tf=True, norm='l2')\n",
    "\n",
    "# Fit and transform the text data to get the TF-IDF matrix\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "X_tfidf_test_comp = tfidf_vectorizer.transform(test_text_cleaned)  # For final predictions\n",
    "\n",
    "print(f\"TF-IDF features shape - Train: {X_tfidf_train.shape}, Test: {X_tfidf_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f303eecfb5b5760",
   "metadata": {},
   "source": [
    "## 6. Creating Price Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7565111332000901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:14:41.041587Z",
     "start_time": "2025-05-31T16:14:40.784479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating price features for training set...\n",
      "Creating price features for testing set...\n",
      "\n",
      "Train price features sample:\n",
      "        price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "114704  43.20           0             1               0   3.788725   \n",
      "250731  24.99           0             1               0   3.257712   \n",
      "152967  22.95           0             1               0   3.175968   \n",
      "4541     8.49           1             0               0   2.250239   \n",
      "142062  24.99           0             1               0   3.257712   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "114704    0.760266     -0.130318  \n",
      "250731    0.495769     -0.326493  \n",
      "152967    0.339856     -0.348470  \n",
      "4541      0.095352     -0.504247  \n",
      "142062    0.495769     -0.326493  \n",
      "\n",
      "Test price features sample:\n",
      "         price  price_0_10  price_10_100  price_100_plus  price_log  \\\n",
      "56218    11.99           0             1               0   2.564180   \n",
      "42346    65.00           0             1               0   4.189655   \n",
      "215842   23.95           0             1               0   3.216874   \n",
      "36062   119.99           0             0               1   4.795708   \n",
      "188250  159.00           0             0               1   5.075174   \n",
      "\n",
      "        price_rank  price_zscore  \n",
      "56218     0.165820     -0.466542  \n",
      "42346     0.832875      0.104533  \n",
      "215842    0.347504     -0.337697  \n",
      "36062     0.912695      0.696937  \n",
      "188250    0.940469      1.117190  \n",
      "Price features shape - Train: (145603, 7), Test: (36401, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create price features for training and testing sets\n",
    "print(\"Creating price features for training set...\")\n",
    "train_price_features = create_price_features(train_product_ids, price_df)\n",
    "\n",
    "print(\"Creating price features for testing set...\")\n",
    "test_price_features = create_price_features(test_product_ids, price_df)\n",
    "\n",
    "# Display the first few rows of price features\n",
    "print(\"\\nTrain price features sample:\")\n",
    "print(train_price_features.head())\n",
    "\n",
    "print(\"\\nTest price features sample:\")\n",
    "print(test_price_features.head())\n",
    "\n",
    "# Scale the price features (except binary features)\n",
    "price_scaler = StandardScaler()\n",
    "price_columns_to_scale = ['price', 'price_log', 'price_rank', 'price_zscore']\n",
    "binary_columns = ['price_0_10', 'price_10_100', 'price_100_plus']\n",
    "\n",
    "# Scale the selected columns\n",
    "train_price_scaled = train_price_features.copy()\n",
    "test_price_scaled = test_price_features.copy()\n",
    "\n",
    "train_price_scaled[price_columns_to_scale] = price_scaler.fit_transform(train_price_features[price_columns_to_scale])\n",
    "test_price_scaled[price_columns_to_scale] = price_scaler.transform(test_price_features[price_columns_to_scale])\n",
    "\n",
    "# Convert to numpy arrays for easier handling\n",
    "train_price_features_array = train_price_scaled.values\n",
    "test_price_features_array = test_price_scaled.values\n",
    "\n",
    "print(f\"Price features shape - Train: {train_price_features_array.shape}, Test: {test_price_features_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10105b25c66f503e",
   "metadata": {},
   "source": [
    "## 7. Combining Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0735ec65d80fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:14:45.477777Z",
     "start_time": "2025-05-31T16:14:43.915833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting node2vec embeddings to sparse format...\n",
      "Converting price features to sparse format...\n",
      "Combining all features...\n",
      "Final combined features shape - Train: (145603, 2060148), Test: (36401, 2060148)\n",
      "Memory usage (approximate):\n",
      "  - X_combined_train: 230.32 MB\n",
      "  - X_combined_test: 55.17 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert node2vec embeddings to sparse for efficient concatenation\n",
    "print(\"Converting node2vec embeddings to sparse format...\")\n",
    "train_node2vec_sparse = csr_matrix(train_node2vec_scaled)\n",
    "test_node2vec_sparse = csr_matrix(test_node2vec_scaled)\n",
    "\n",
    "# Convert price features to sparse\n",
    "print(\"Converting price features to sparse format...\")\n",
    "train_price_sparse = csr_matrix(train_price_features_array)\n",
    "test_price_sparse = csr_matrix(test_price_features_array)\n",
    "\n",
    "# Combine all features: TF-IDF + graph features + node2vec embeddings + price features\n",
    "print(\"Combining all features...\")\n",
    "X_combined_train = hstack([X_tfidf_train, train_node2vec_sparse, train_price_sparse], format='csr')\n",
    "X_combined_test = hstack([X_tfidf_test, test_node2vec_sparse, test_price_sparse], format='csr')\n",
    "\n",
    "# Output memory usage information\n",
    "print(f\"Final combined features shape - Train: {X_combined_train.shape}, Test: {X_combined_test.shape}\")\n",
    "print(\"Memory usage (approximate):\")\n",
    "print(f\"  - X_combined_train: {X_combined_train.data.nbytes / (1024 ** 2):.2f} MB\")\n",
    "print(f\"  - X_combined_test: {X_combined_test.data.nbytes / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f33059d218a16",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7a5d54-903e-4179-b817-af7b92dfad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1000000 features\n",
      "(145603, 1000000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "k_feat = 1000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected = feature_selector.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected = feature_selector.transform(X_combined_test)\n",
    "print(X_combined_train_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61619817-cb24-47ea-84d8-6dcc9bb7ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 2000000 features\n",
      "(145603, 2000000)\n"
     ]
    }
   ],
   "source": [
    "# We trained the LinearSVC on 2m features so we use another feature selector\n",
    "k_feat = 2000000\n",
    "print(f\"Selecting {k_feat} features\")\n",
    "feature_selector_svc = SelectKBest(f_classif, k=k_feat)\n",
    "X_combined_train_selected_svc = feature_selector_svc.fit_transform(X_combined_train, y_train)\n",
    "X_combined_test_selected_svc = feature_selector_svc.transform(X_combined_test)\n",
    "print(X_combined_train_selected_svc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b789445531b96a2",
   "metadata": {},
   "source": [
    "### 8.1 Calibrated LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af5473-0cfa-4cf2-9507-a42f189ae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'linear_svc_model.pkl'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing LinearSVC model...\")\n",
    "    svc_model = joblib.load(model_path)\n",
    "else:\n",
    "    print(\"Training Calibrated LinearSVC model...\")\n",
    "    cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "    base_svc = LinearSVC(max_iter=10000, dual=False)\n",
    "    svc_model = CalibratedClassifierCV(base_svc, cv=cv, method='isotonic')  # Using isotonic calibration\n",
    "    svc_model.fit(X_combined_train_selected_svc, y_train)\n",
    "    joblib.dump(svc_model, model_path)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svc = svc_model.predict(X_combined_test_selected_svc)\n",
    "y_proba_svc = svc_model.predict_proba(X_combined_test_selected_svc)\n",
    "\n",
    "# Evaluate\n",
    "svc_accuracy = accuracy_score(y_test, y_pred_svc)\n",
    "svc_log_loss = multiclass_log_loss(y_test, y_proba_svc)\n",
    "print(f\"LinearSVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"LinearSVC Log Loss: {svc_log_loss:.4f}\")\n",
    "print(\"\\nLinearSVC Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4000cb42a02af",
   "metadata": {},
   "source": [
    "### 8.2 XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3bfee-cf4a-4a18-a5dc-b1b5b22e3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or Load XGBoost model\n",
    "\n",
    "\n",
    "model_path = 'xgb_model.pkl'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing XGBoost model...\")\n",
    "    xgb_model = joblib.load(model_path)\n",
    "else:\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_initial = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',  # Needed for multiclass probability output\n",
    "        num_class=len(np.unique(y_train)),  # Set number of classes\n",
    "        eval_metric='mlogloss',  # Use multiclass log loss as eval metric\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb_model = CalibratedClassifierCV(xgb_initial, method='isotonic', cv=5)\n",
    "    xgb_model.fit(X_combined_train_selected, y_train)\n",
    "    joblib.dump(xgb_model, model_path)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_combined_test_selected)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_combined_test_selected)\n",
    "\n",
    "# Evaluate\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_log_loss = multiclass_log_loss(y_test, y_proba_xgb)\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"XGBoost Log Loss: {xgb_log_loss:.4f}\")\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05a89c4526bb4a",
   "metadata": {},
   "source": [
    "### 8.3 Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739268adf74d5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or Load Neural Network model\n",
    "model_path = 'neural_network_model.keras'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing Neural Network model...\")\n",
    "    nn_model = load_model(model_path)\n",
    "else:\n",
    "    # Get number of classes\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Build neural network model\n",
    "    print(\"Building Neural Network model with selected features...\")\n",
    "    nn_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_combined_train_selected.shape[1],)),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    nn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training Neural Network model...\")\n",
    "    history = nn_model.fit(\n",
    "        X_combined_train_selected, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    nn_model.save(model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = nn_model.predict(X_combined_test_selected)\n",
    "y_pred_nn_classes = np.argmax(y_pred_nn, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn_classes)\n",
    "nn_log_loss = multiclass_log_loss(y_test, y_pred_nn)\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy:.4f}\")\n",
    "print(f\"Neural Network Log Loss: {nn_log_loss:.4f}\")\n",
    "print(\"\\nNeural Network Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802502f49a5be5d",
   "metadata": {},
   "source": [
    "### 8.4 Ensemble Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8afb73-4090-4a6b-854f-f6917c18ba64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 66 weight combinations...\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE WEIGHT COMBINATIONS RANKED BY PERFORMANCE\n",
      "================================================================================\n",
      "Rank   SVM    XGB    NN     Log Loss     Improvement \n",
      "--------------------------------------------------------------------------------\n",
      "1      0.6    0.2    0.2    0.199499     BEST        \n",
      "2      0.7    0.2    0.1    0.199950     +0.23%      \n",
      "3      0.5    0.2    0.3    0.200082     +0.29%      \n",
      "4      0.7    0.1    0.2    0.200177     +0.34%      \n",
      "5      0.6    0.1    0.3    0.200205     +0.35%      \n",
      "6      0.6    0.3    0.1    0.200235     +0.37%      \n",
      "7      0.5    0.3    0.2    0.200337     +0.42%      \n",
      "8      0.5    0.1    0.4    0.201225     +0.87%      \n",
      "9      0.8    0.1    0.1    0.201240     +0.87%      \n",
      "10     0.7    0.3    -0.0   0.201440     +0.97%      \n",
      "11     0.4    0.3    0.3    0.201506     +1.01%      \n",
      "12     0.4    0.2    0.4    0.201692     +1.10%      \n",
      "13     0.5    0.4    0.1    0.201705     +1.11%      \n",
      "14     0.8    0.2    -0.0   0.201831     +1.17%      \n",
      "15     0.6    0.4    -0.0   0.202306     +1.41%      \n",
      "16     0.4    0.4    0.2    0.202396     +1.45%      \n",
      "17     0.7    0.0    0.3    0.203241     +1.88%      \n",
      "18     0.4    0.1    0.5    0.203309     +1.91%      \n",
      "19     0.6    0.0    0.4    0.203657     +2.08%      \n",
      "20     0.3    0.3    0.4    0.203831     +2.17%      \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate weight combinations\n",
    "weight_combinations = []\n",
    "results = []\n",
    "\n",
    "# Generate all combinations where weights sum to 1.0 (with 0.1 increments)\n",
    "for w1 in np.arange(0.0, 1.1, 0.1):\n",
    "    for w2 in np.arange(0.0, 1.1 - w1, 0.1):\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        if abs(w3 - round(w3, 1)) < 1e-10: \n",
    "            w3 = round(w3, 1)\n",
    "            if w3 >= 0:\n",
    "                weight_combinations.append([round(w1, 1), round(w2, 1), w3])\n",
    "\n",
    "print(f\"Testing {len(weight_combinations)} weight combinations...\")\n",
    "\n",
    "# Test each combination\n",
    "for i, weights in enumerate(weight_combinations):\n",
    "    y_proba_ensemble = (\n",
    "        weights[0] * y_proba_svc + \n",
    "        weights[1] * y_proba_xgb + \n",
    "        weights[2] * y_pred_nn\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        ensemble_log_loss = multiclass_log_loss(y_test, y_proba_ensemble)\n",
    "        results.append({\n",
    "            'weights': weights.copy(),\n",
    "            'svc_weight': weights[0],\n",
    "            'xgb_weight': weights[1], \n",
    "            'nn_weight': weights[2],\n",
    "            'log_loss': ensemble_log_loss\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "results.sort(key=lambda x: x['log_loss'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE WEIGHT COMBINATIONS RANKED BY PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rank':<6} {'SVC':<6} {'XGB':<6} {'NN':<6} {'Log Loss':<12} {'Improvement':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_log_loss = results[0]['log_loss']\n",
    "\n",
    "for i, result in enumerate(results[:20]):  # Show top 20 results\n",
    "    improvement = \"\"\n",
    "    if i > 0:\n",
    "        improvement = f\"{((results[i]['log_loss'] - best_log_loss) / best_log_loss * 100):+.2f}%\"\n",
    "    else:\n",
    "        improvement = \"BEST\"\n",
    "    \n",
    "    print(f\"{i+1:<6} {result['svc_weight']:<6.1f} {result['xgb_weight']:<6.1f} \"\n",
    "          f\"{result['nn_weight']:<6.1f} {result['log_loss']:<12.6f} {improvement:<12}\")\n",
    "\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8850933-4191-44c0-814e-c8898b74eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING OPTIMAL WEIGHTS AUTOMATICALLY\n",
      "Best weights found: SVM=0.6, XGB=0.2, NN=0.2\n",
      "Updated ensemble weights: SVM=0.60, XGB=0.20, NN=0.20\n"
     ]
    }
   ],
   "source": [
    "# Automatically keep and apply the best weights\n",
    "print(\"APPLYING OPTIMAL WEIGHTS AUTOMATICALLY\")\n",
    "\n",
    "# Get the best weights from our optimization\n",
    "best_weights = results[0]['weights']\n",
    "print(f\"Best weights found: SVC={best_weights[0]:.1f}, XGB={best_weights[1]:.1f}, NN={best_weights[2]:.1f}\")\n",
    "\n",
    "weights = best_weights.copy()\n",
    "print(f\"Updated ensemble weights: SVC={weights[0]:.2f}, XGB={weights[1]:.2f}, NN={weights[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56f15c2b-1ce2-4600-8278-c011aa771799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ensemble model...\n",
      "Ensemble weights: SVM=0.60, XGB=0.20, NN=0.20\n",
      "Ensemble Accuracy: 0.9431\n",
      "Ensemble Log Loss: 0.1995\n",
      "\n",
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3033\n",
      "           1       0.91      0.91      0.91      2372\n",
      "           2       0.93      0.95      0.94      8652\n",
      "           3       0.96      0.97      0.96      1073\n",
      "           4       0.95      0.96      0.96      3016\n",
      "           5       0.97      0.97      0.97      3564\n",
      "           6       0.96      0.96      0.96      1519\n",
      "           7       0.96      0.94      0.95      3752\n",
      "           8       0.97      0.97      0.97      1316\n",
      "           9       0.95      0.95      0.95       903\n",
      "          10       0.89      0.89      0.89      3589\n",
      "          11       0.94      0.93      0.93      1425\n",
      "          12       0.89      0.84      0.87      1318\n",
      "          13       0.92      0.87      0.90       323\n",
      "          14       0.97      0.97      0.97       226\n",
      "          15       0.98      0.94      0.96       320\n",
      "\n",
      "    accuracy                           0.94     36401\n",
      "   macro avg       0.95      0.94      0.94     36401\n",
      "weighted avg       0.94      0.94      0.94     36401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an ensemble model using the three models\n",
    "print(\"Creating ensemble model...\")\n",
    "\n",
    "print(f\"Ensemble weights: SVC={weights[0]:.2f}, XGB={weights[1]:.2f}, NN={weights[2]:.2f}\")\n",
    "\n",
    "# Combine predictions with weighted average\n",
    "y_proba_ensemble = (\n",
    "    weights[0] * y_proba_svc + \n",
    "    weights[1] * y_proba_xgb + \n",
    "    weights[2] * y_pred_nn\n",
    ")\n",
    "\n",
    "# Get class predictions\n",
    "y_pred_ensemble = np.argmax(y_proba_ensemble, axis=1)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_log_loss = multiclass_log_loss(y_test, y_proba_ensemble)\n",
    "\n",
    "print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble Log Loss: {ensemble_log_loss:.4f}\")\n",
    "print(\"\\nEnsemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269844c0a295bae",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions for Test Products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b47835a8b57567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating features for test products...\n",
      "\n",
      "Extracting graph features for test products...\n",
      "Calculating degree centrality...\n",
      "Calculating clustering coefficient...\n",
      "Calculating PageRank...\n",
      "Calculating triangle count...\n",
      "Calculating degree...\n",
      "Graph features shape for test products: (45502, 5)\n",
      "Node2Vec features shape for test products: (45502, 128)\n",
      "Price features shape for test products: (45502, 7)\n",
      "\n",
      "Combining all features...\n",
      "Combined features shape for test products: (45502, 2060153)\n",
      "\n",
      "Making predictions using all models...\n",
      "\u001b[1m1422/1422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 149ms/step\n",
      "Predictions shape: (45502, 16)\n",
      "Predictions saved to predictions.csv\n",
      "\n",
      "Sample of predictions:\n",
      "  product  class0  class1  class2  class3  class4  class5  class6  class7  \\\n",
      "0   49957  0.0003  0.0038  0.0040  0.0002  0.0020  0.0701  0.0152  0.0006   \n",
      "1  135386  0.0001  0.0004  0.0001  0.0000  0.0008  0.0465  0.0004  0.0014   \n",
      "2  226880  0.0000  0.0007  0.9912  0.0001  0.0031  0.0005  0.0002  0.0017   \n",
      "3  165114  0.0000  0.0021  0.0127  0.0001  0.0074  0.0002  0.0003  0.0012   \n",
      "4  256154  0.0000  0.0008  0.9878  0.0001  0.0008  0.0003  0.0010  0.0012   \n",
      "\n",
      "   class8  class9  class10  class11  class12  class13  class14  class15  \n",
      "0  0.0002  0.0002   0.8987   0.0010   0.0030   0.0002   0.0000   0.0005  \n",
      "1  0.0001  0.9493   0.0004   0.0001   0.0002   0.0001   0.0001   0.0000  \n",
      "2  0.0001  0.0001   0.0016   0.0002   0.0004   0.0000   0.0001   0.0001  \n",
      "3  0.0004  0.0008   0.9740   0.0002   0.0000   0.0001   0.0000   0.0004  \n",
      "4  0.0001  0.0001   0.0005   0.0008   0.0061   0.0001   0.0002   0.0001  \n"
     ]
    }
   ],
   "source": [
    "# Create features for test products\n",
    "print(\"\\nCreating features for test products...\")\n",
    "\n",
    "# 1. Create Node2Vec embeddings for test products\n",
    "test_node2vec_features_final = np.zeros((len(test_products), 128))\n",
    "for i, node_id in enumerate(test_products):\n",
    "    try:\n",
    "        test_node2vec_features_final[i] = n2v_model.wv[str(node_id)]\n",
    "    except KeyError:\n",
    "        # If node not in embeddings, use zeros\n",
    "        pass\n",
    "\n",
    "# Scale the Node2Vec embeddings\n",
    "test_node2vec_scaled_final = n2v_scaler.transform(test_node2vec_features_final)\n",
    "test_node2vec_sparse_final = csr_matrix(test_node2vec_scaled_final)\n",
    "print(f\"Node2Vec features shape for test products: {test_node2vec_sparse_final.shape}\")\n",
    "\n",
    "# 2. Create price features for test products\n",
    "test_price_features_final = create_price_features(test_products, price_df)\n",
    "test_price_scaled_final = test_price_features_final.copy()\n",
    "test_price_scaled_final[price_columns_to_scale] = price_scaler.transform(test_price_features_final[price_columns_to_scale])\n",
    "test_price_features_array_final = test_price_scaled_final.values\n",
    "test_price_sparse_final = csr_matrix(test_price_features_array_final)\n",
    "print(f\"Price features shape for test products: {test_price_sparse_final.shape}\")\n",
    "\n",
    "# COMBINE ALL FEATURES\n",
    "print(\"\\nCombining all features...\")\n",
    "X_combined_test_final = hstack([\n",
    "    X_tfidf_test_comp,\n",
    "    test_node2vec_sparse_final, \n",
    "    test_price_sparse_final\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Combined features shape for test products: {X_combined_test_final.shape}\")\n",
    "\n",
    "# Apply feature selection for neural network\n",
    "X_combined_test_final_selected = feature_selector.transform((X_combined_test_final))\n",
    "X_combined_test_final_selected_svc = feature_selector_svc.transform((X_combined_test_final))\n",
    "\n",
    "# Make predictions using all models\n",
    "print(\"\\nMaking predictions using all models...\")\n",
    "test_pred_proba_svc = svc_model.predict_proba(X_combined_test_final_selected_svc)\n",
    "test_pred_proba_xgb = xgb_model.predict_proba(X_combined_test_final_selected)\n",
    "test_pred_proba_nn = nn_model.predict(X_combined_test_final_selected)\n",
    "\n",
    "# Combine predictions with weighted average\n",
    "test_pred_proba_ensemble = (\n",
    "    weights[0] * test_pred_proba_svc + \n",
    "    weights[1] * test_pred_proba_xgb + \n",
    "    weights[2] * test_pred_proba_nn\n",
    ")\n",
    "\n",
    "print(f\"Predictions shape: {test_pred_proba_ensemble.shape}\")\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['product'] = test_products\n",
    "\n",
    "# Add probability for each class\n",
    "for i in range(test_pred_proba_ensemble.shape[1]):\n",
    "    predictions_df[f'class{i}'] = test_pred_proba_ensemble[:, i].round(4)\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(f\"Predictions saved to predictions.csv\")\n",
    "\n",
    "# Display the first few rows of the predictions\n",
    "print(\"\\nSample of predictions:\")\n",
    "print(predictions_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
